<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Analyse Everything You See</title><description>Data Made Simple</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Analyse Everything You See</title><link>http://localhost:2368/</link></image><generator>Ghost 3.8</generator><lastBuildDate>Thu, 02 Apr 2020 16:50:11 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Mean Median Mode</title><description>&lt;p&gt;Mean, median, mode. - Draft&lt;/p&gt;</description><link>http://localhost:2368/mean-median-mode/</link><guid isPermaLink="false">5e8616ea3359460b34f167ec</guid><dc:creator>Nagaraj Sundaramahalingam</dc:creator><pubDate>Thu, 02 Apr 2020 16:46:57 GMT</pubDate><content:encoded>&lt;p&gt;Mean, median, mode. - Draft&lt;/p&gt;</content:encoded></item><item><title>Beer Me!</title><description>In this article, I take a data set of ~75,000 homemade beer recipes and see if i can predict the style of beer.</description><link>http://localhost:2368/beer-me/</link><guid isPermaLink="false">5e7ba6e1597447036c707ecb</guid><category>Machine Learning</category><category>R</category><dc:creator>Gary Clark Jr.</dc:creator><pubDate>Wed, 25 Mar 2020 23:48:51 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1504502350688-00f5d59bbdeb?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded>&lt;!--kg-card-begin: markdown--&gt;&lt;h3 id="context"&gt;Context&lt;/h3&gt;
&lt;img src="https://images.unsplash.com/photo-1504502350688-00f5d59bbdeb?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Beer Me!"&gt;&lt;p&gt;This is a dataset of 75,000 homebrewed beers with over 176 different styles. Beer records are user-reported and are classified according to one of the 176 different styles. These recipes go into as much or as little detail as the user provided, but there's at least 5 useful columns where data was entered for each: Original Gravity, Final Gravity, ABV, IBU, and Color&lt;/p&gt;
&lt;h3 id="inspiration"&gt;Inspiration&lt;/h3&gt;
&lt;p&gt;What goes into homemade beer?&lt;/p&gt;
&lt;p&gt;It would be interesting to see if the data provided is enough to define each class or if there are undiscovered patterns. In the future it might be possible to go through and scrape more detailed information for each recipe, such as the yeast and specific hops used.&lt;/p&gt;
&lt;h2 id="problemstatement"&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;Based on the above, let's attempt to predict beer style.&lt;/p&gt;
&lt;h2 id="intitialsetup"&gt;Intitial Setup&lt;/h2&gt;
&lt;h3 id="clearenvironment"&gt;Clear Environment&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;rm(list = ls())

options(repr.plot.width=13, repr.plot.height=10)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="loadlibraries"&gt;Load Libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;library(tidyverse)
library(tidymodels)
library(skimr)
library(GGally)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="readindata"&gt;Read in Data&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_recipes = read_csv(&amp;quot;recipeData.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="intitialview"&gt;Intitial View&lt;/h1&gt;
&lt;p&gt;Let's get a glimpse of what're working with here.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_recipes %&amp;gt;% glimpse()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 73,861
Variables: 23
$ BeerID        &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …
$ Name          &amp;lt;chr&amp;gt; &amp;quot;Vanilla Cream Ale&amp;quot;, &amp;quot;Southern Tier Pumking clone&amp;quot;, &amp;quot;Zo…
$ URL           &amp;lt;chr&amp;gt; &amp;quot;/homebrew/recipe/view/1633/vanilla-cream-ale&amp;quot;, &amp;quot;/homeb…
$ Style         &amp;lt;chr&amp;gt; &amp;quot;Cream Ale&amp;quot;, &amp;quot;Holiday/Winter Special Spiced Beer&amp;quot;, &amp;quot;Ame…
$ StyleID       &amp;lt;dbl&amp;gt; 45, 85, 7, 7, 20, 10, 86, 45, 129, 86, 7, 7, 7, 7, 7, 3…
$ `Size(L)`     &amp;lt;dbl&amp;gt; 21.77, 20.82, 18.93, 22.71, 50.00, 24.61, 22.71, 20.82,…
$ OG            &amp;lt;dbl&amp;gt; 1.055, 1.083, 1.063, 1.061, 1.060, 1.055, 1.072, 1.054,…
$ FG            &amp;lt;dbl&amp;gt; 1.013, 1.021, 1.018, 1.017, 1.010, 1.013, 1.018, 1.014,…
$ ABV           &amp;lt;dbl&amp;gt; 5.48, 8.16, 5.91, 5.80, 6.48, 5.58, 7.09, 5.36, 5.77, 8…
$ IBU           &amp;lt;dbl&amp;gt; 17.65, 60.65, 59.25, 54.48, 17.84, 40.12, 268.71, 19.97…
$ Color         &amp;lt;dbl&amp;gt; 4.83, 15.64, 8.98, 8.50, 4.57, 8.00, 6.33, 5.94, 34.76,…
$ BoilSize      &amp;lt;dbl&amp;gt; 28.39, 24.61, 22.71, 26.50, 60.00, 29.34, 30.28, 28.39,…
$ BoilTime      &amp;lt;dbl&amp;gt; 75, 60, 60, 60, 90, 70, 90, 75, 75, 60, 90, 90, 60, 60,…
$ BoilGravity   &amp;lt;chr&amp;gt; &amp;quot;1.038&amp;quot;, &amp;quot;1.07&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;1.05&amp;quot;, &amp;quot;1.047&amp;quot;, &amp;quot;N/A&amp;quot;, …
$ Efficiency    &amp;lt;dbl&amp;gt; 70, 70, 70, 70, 72, 79, 75, 70, 73, 70, 74, 70, 70, 30,…
$ MashThickness &amp;lt;chr&amp;gt; &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;1.4&amp;quot;,…
$ SugarScale    &amp;lt;chr&amp;gt; &amp;quot;Specific Gravity&amp;quot;, &amp;quot;Specific Gravity&amp;quot;, &amp;quot;Specific Gravi…
$ BrewMethod    &amp;lt;chr&amp;gt; &amp;quot;All Grain&amp;quot;, &amp;quot;All Grain&amp;quot;, &amp;quot;extract&amp;quot;, &amp;quot;All Grain&amp;quot;, &amp;quot;All …
$ PitchRate     &amp;lt;chr&amp;gt; &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;…
$ PrimaryTemp   &amp;lt;chr&amp;gt; &amp;quot;17.78&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;19&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;…
$ PrimingMethod &amp;lt;chr&amp;gt; &amp;quot;corn sugar&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;Sukkerlake&amp;quot;, &amp;quot;N/A&amp;quot;,…
$ PrimingAmount &amp;lt;chr&amp;gt; &amp;quot;4.5 oz&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;N/A&amp;quot;, &amp;quot;6-7 g sukker/l&amp;quot;, &amp;quot;N/A&amp;quot;,…
$ UserId        &amp;lt;dbl&amp;gt; 116, 955, NA, NA, 18325, 5889, 1051, 116, 116, NA, 1832…
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! I see a few issues in the data that I already know I'm going to have to account for, but I'll go into more detail when I get there.&lt;/p&gt;
&lt;h1 id="datacleansing"&gt;Data Cleansing&lt;/h1&gt;
&lt;h2 id="usingarecipe"&gt;Using a Recipe&lt;/h2&gt;
&lt;p&gt;In R, there is a concept called a &lt;strong&gt;recipe&lt;/strong&gt; (nothing to do with the data I'm currently working with lol). It basically allows one to build incremental steps for cleaning/pre-processing the data to get it ready for analysis, modeling, etc. I personally like it because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It will list out the steps that I am performing on the data, and what variables are going to be affected by those steps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It performs lazy evaluation, so it doesn't actually do anything to the data until you ask for it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So let's start off with initializing our recipe!&lt;/p&gt;
&lt;p&gt;At this point I'm going to split our train and test set, our train set will guide us through all of the steps we need to perform in order to get our model ready, and our test set will act as new data that we've never seen before to help assess how well our model did on &lt;strong&gt;real data&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;set.seed(0415)
split = initial_split(beer_recipes)

split

train = training(split)
test = testing(split)

beer_rec = train %&amp;gt;% recipe()

beer_rec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;55396/18465/73861&amp;gt;



Data Recipe

Inputs:

  23 variables (no declared roles)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So ~55,000 training records and ~18,000 testing records.&lt;/p&gt;
&lt;p&gt;Some of the first things that stood out to me in the glimpse above are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;There are values showing up as &amp;quot;N/A&amp;quot; and not being recognized as &lt;strong&gt;actual NA&lt;/strong&gt; values in R.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BoilGravity and PrimaryTemp columns are character columns when they should be numeric. This is likely due to the &amp;quot;N/A&amp;quot; issue.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additionally, there are some standard things I'll take care of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Change all column names to lower case. This is mostly a matter of preference.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change all character columns to factors (categorical). This is to help reduce memory.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_rename_at(everything(), fn = ~str_to_lower(.)) %&amp;gt;% # change column names to lowercase
    step_rename(size_l = `size(l)`) %&amp;gt;%
    step_mutate_at(all_nominal(), fn = ~na_if(., &amp;quot;N/A&amp;quot;)) %&amp;gt;% # convert &amp;quot;N/A&amp;quot; to NA
    step_mutate_at(boilgravity, primarytemp, fn = ~as.numeric(as.character(.)))

beer_rec
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see above that the output of our recipe provides a nice simple way to understand what steps and in what order the recipe intends to perform on our data.&lt;/p&gt;
&lt;p&gt;If we wanted to get an idea of what our data would look like as a result of our recipe &lt;strong&gt;so far&lt;/strong&gt; we can do that. Just prepare the recipe and juice it!&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% glimpse()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 55,396
Variables: 23
$ beerid        &amp;lt;dbl&amp;gt; 1, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 16, 17, 20, 21, 22…
$ name          &amp;lt;fct&amp;gt; Vanilla Cream Ale, Zombie Dust Clone - EXTRACT, Zombie …
$ url           &amp;lt;fct&amp;gt; /homebrew/recipe/view/1633/vanilla-cream-ale, /homebrew…
$ style         &amp;lt;fct&amp;gt; Cream Ale, American IPA, American IPA, Belgian Blond Al…
$ styleid       &amp;lt;dbl&amp;gt; 45, 7, 7, 20, 10, 86, 45, 7, 7, 7, 7, 31, 134, 7, 65, 8…
$ size_l        &amp;lt;dbl&amp;gt; 21.77, 18.93, 22.71, 50.00, 24.61, 22.71, 20.82, 25.00,…
$ og            &amp;lt;dbl&amp;gt; 1.0550, 1.0630, 1.0610, 1.0600, 1.0550, 1.0720, 1.0540,…
$ fg            &amp;lt;dbl&amp;gt; 1.0130, 1.0180, 1.0170, 1.0100, 1.0130, 1.0180, 1.0140,…
$ abv           &amp;lt;dbl&amp;gt; 5.48, 5.91, 5.80, 6.48, 5.58, 7.09, 5.36, 6.63, 6.62, 7…
$ ibu           &amp;lt;dbl&amp;gt; 17.65, 59.25, 54.48, 17.84, 40.12, 268.71, 19.97, 64.26…
$ color         &amp;lt;dbl&amp;gt; 4.83, 8.98, 8.50, 4.57, 8.00, 6.33, 5.94, 7.78, 14.26, …
$ boilsize      &amp;lt;dbl&amp;gt; 28.39, 22.71, 26.50, 60.00, 29.34, 30.28, 28.39, 29.00,…
$ boiltime      &amp;lt;dbl&amp;gt; 75, 60, 60, 90, 70, 90, 75, 90, 90, 60, 60, 60, 60, 60,…
$ boilgravity   &amp;lt;dbl&amp;gt; 1.038, NA, NA, 1.050, 1.047, NA, 1.040, 1.055, NA, NA, …
$ efficiency    &amp;lt;dbl&amp;gt; 70, 70, 70, 72, 79, 75, 70, 74, 70, 70, 30, 75, 70, 70,…
$ mashthickness &amp;lt;fct&amp;gt; NA, NA, NA, NA, NA, NA, 1.4, NA, NA, NA, NA, NA, NA, 1.…
$ sugarscale    &amp;lt;fct&amp;gt; Specific Gravity, Specific Gravity, Specific Gravity, S…
$ brewmethod    &amp;lt;fct&amp;gt; All Grain, extract, All Grain, All Grain, All Grain, Al…
$ pitchrate     &amp;lt;fct&amp;gt; NA, NA, NA, NA, 1, NA, NA, 1, NA, NA, 0.35, NA, NA, NA,…
$ primarytemp   &amp;lt;dbl&amp;gt; 17.78, NA, NA, 19.00, NA, NA, NA, 18.00, NA, NA, 18.33,…
$ primingmethod &amp;lt;fct&amp;gt; corn sugar, NA, NA, Sukkerlake, NA, NA, corn sugar, Suk…
$ primingamount &amp;lt;fct&amp;gt; 4.5 oz, NA, NA, 6-7 g sukker/l, NA, NA, 4.2 oz, 6 5 g s…
$ userid        &amp;lt;dbl&amp;gt; 116, NA, NA, 18325, 5889, 1051, 116, 18325, NA, NA, 235…
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are some columns in here that don't provide any additional value to us since they are essentially id columns, or made redundant by other columns:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Beerid is the id of every row&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Name is going to be mostly unique to every row (this could prove useful to analyze, but for now I'll drop it)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;URL is just the link to where the recipe lives&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Styleid is already represented by Style&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Userid is a variable that could have some value (particular users are more likely to input their favorite beers) but we will disregard it for now&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_rm(
        beerid,
        name,
        url,
        styleid,
        userid
    ) # remove the variables mentioned above
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec
beer_rec %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% glimpse()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid


Observations: 55,396
Variables: 18
$ style         &amp;lt;fct&amp;gt; Cream Ale, American IPA, American IPA, Belgian Blond Al…
$ size_l        &amp;lt;dbl&amp;gt; 21.77, 18.93, 22.71, 50.00, 24.61, 22.71, 20.82, 25.00,…
$ og            &amp;lt;dbl&amp;gt; 1.0550, 1.0630, 1.0610, 1.0600, 1.0550, 1.0720, 1.0540,…
$ fg            &amp;lt;dbl&amp;gt; 1.0130, 1.0180, 1.0170, 1.0100, 1.0130, 1.0180, 1.0140,…
$ abv           &amp;lt;dbl&amp;gt; 5.48, 5.91, 5.80, 6.48, 5.58, 7.09, 5.36, 6.63, 6.62, 7…
$ ibu           &amp;lt;dbl&amp;gt; 17.65, 59.25, 54.48, 17.84, 40.12, 268.71, 19.97, 64.26…
$ color         &amp;lt;dbl&amp;gt; 4.83, 8.98, 8.50, 4.57, 8.00, 6.33, 5.94, 7.78, 14.26, …
$ boilsize      &amp;lt;dbl&amp;gt; 28.39, 22.71, 26.50, 60.00, 29.34, 30.28, 28.39, 29.00,…
$ boiltime      &amp;lt;dbl&amp;gt; 75, 60, 60, 90, 70, 90, 75, 90, 90, 60, 60, 60, 60, 60,…
$ boilgravity   &amp;lt;dbl&amp;gt; 1.038, NA, NA, 1.050, 1.047, NA, 1.040, 1.055, NA, NA, …
$ efficiency    &amp;lt;dbl&amp;gt; 70, 70, 70, 72, 79, 75, 70, 74, 70, 70, 30, 75, 70, 70,…
$ mashthickness &amp;lt;fct&amp;gt; NA, NA, NA, NA, NA, NA, 1.4, NA, NA, NA, NA, NA, NA, 1.…
$ sugarscale    &amp;lt;fct&amp;gt; Specific Gravity, Specific Gravity, Specific Gravity, S…
$ brewmethod    &amp;lt;fct&amp;gt; All Grain, extract, All Grain, All Grain, All Grain, Al…
$ pitchrate     &amp;lt;fct&amp;gt; NA, NA, NA, NA, 1, NA, NA, 1, NA, NA, 0.35, NA, NA, NA,…
$ primarytemp   &amp;lt;dbl&amp;gt; 17.78, NA, NA, 19.00, NA, NA, NA, 18.00, NA, NA, 18.33,…
$ primingmethod &amp;lt;fct&amp;gt; corn sugar, NA, NA, Sukkerlake, NA, NA, corn sugar, Suk…
$ primingamount &amp;lt;fct&amp;gt; 4.5 oz, NA, NA, 6-7 g sukker/l, NA, NA, 4.2 oz, 6 5 g s…
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="exploration"&gt;Exploration&lt;/h1&gt;
&lt;p&gt;Now lets get a look at the quality of the data that remains. The skimr package has a function, skim, that provides a nice  30,000 ft view.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_skim = beer_rec %&amp;gt;%
    prep() %&amp;gt;%
    juice() %&amp;gt;%
    skim()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_skim
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;── Data Summary ────────────────────────
                           Values    
Name                       Piped data
Number of rows             55396     
Number of columns          18        
_______________________              
Column type frequency:               
  factor                   7         
  numeric                  11        
________________________             
Group variables            None      

── Variable type: factor ───────────────────────────────────────────────────────
  skim_variable n_missing complete_rate ordered n_unique
1 style               465        0.992  FALSE        175
2 mashthickness     22380        0.596  FALSE        496
3 sugarscale            0        1      FALSE          2
4 brewmethod            0        1      FALSE          4
5 pitchrate         29239        0.472  FALSE          9
6 primingmethod     50274        0.0925 FALSE        722
7 primingamount     51802        0.0649 FALSE       1559
  top_counts                                 
1 Ame: 8968, Ame: 5677, Sai: 1953, Ame: 1678 
2 1.5: 11560, 3: 6296, 1.2: 3679, 2.5: 1421  
3 Spe: 53976, Pla: 1420                      
4 All: 37264, BIA: 8908, ext: 6522, Par: 2702
5 0.3: 7139, 0.7: 6840, 0.5: 4119, 1: 3992   
6 Cor: 546, Dex: 389, cor: 267, Keg: 247     
7 5 o: 153, 3/4: 79, 4 o: 78, 1 c: 74        

── Variable type: numeric ──────────────────────────────────────────────────────
   skim_variable n_missing complete_rate  mean      sd      p0   p25   p50   p75
 1 size_l                0         1     43.4  174.      1     18.9  20.8  23.7 
 2 og                    0         1      1.40   2.18    1      1.05  1.06  1.07
 3 fg                    0         1      1.08   0.432  -0.003  1.01  1.01  1.02
 4 abv                   0         1      6.14   1.90    0      5.08  5.79  6.83
 5 ibu                   0         1     44.3   43.3     0     23.4  35.8  56.4 
 6 color                 0         1     13.4   11.9     0      5.17  8.44 16.8 
 7 boilsize              0         1     49.2  186.      1     20.8  27.4  30   
 8 boiltime              0         1     65.1   15.1     0     60    60    60   
 9 boilgravity        2246         0.959  1.35   1.92    0      1.04  1.05  1.06
10 efficiency            0         1     66.3   14.1     0     65    70    75   
11 primarytemp       16963         0.694 19.2    4.21  -17.8   18    20    20   
     p100 hist 
 1 7800   ▇▁▁▁▁
 2   32.5 ▇▁▁▁▁
 3   23.4 ▇▁▁▁▁
 4   54.7 ▇▁▁▁▁
 5 3409.  ▇▁▁▁▁
 6  186   ▇▁▁▁▁
 7 7800   ▇▁▁▁▁
 8  240   ▁▇▁▁▁
 9   52.6 ▇▁▁▁▁
10  100   ▁▂▁▇▁
11  114   ▁▇▁▁▁
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One last cleanup step we can do is removing variables with a lot of missing data. Several variables from above have low completion rates. Lets set a threshold of 60%, meaning we will remove variables with less than 60% completion rates. This is a personal threshold that I initially choose to set. This threshold could very well be adjusted.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;vars_remove = beer_skim %&amp;gt;%
    filter(complete_rate &amp;lt; 0.6) %&amp;gt;% # collect variables with less than 60% completion rate
    select(skim_variable) %&amp;gt;%
    .[[1]]

beer_rec = beer_rec %&amp;gt;%
    step_rm(one_of(vars_remove))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec
beer_rec %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% skim()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid
Delete terms one_of, vars_remove


── Data Summary ────────────────────────
                           Values    
Name                       Piped data
Number of rows             55396     
Number of columns          14        
_______________________              
Column type frequency:               
  factor                   3         
  numeric                  11        
________________________             
Group variables            None      

── Variable type: factor ───────────────────────────────────────────────────────
  skim_variable n_missing complete_rate ordered n_unique
1 style               465         0.992 FALSE        175
2 sugarscale            0         1     FALSE          2
3 brewmethod            0         1     FALSE          4
  top_counts                                 
1 Ame: 8968, Ame: 5677, Sai: 1953, Ame: 1678 
2 Spe: 53976, Pla: 1420                      
3 All: 37264, BIA: 8908, ext: 6522, Par: 2702

── Variable type: numeric ──────────────────────────────────────────────────────
   skim_variable n_missing complete_rate  mean      sd      p0   p25   p50   p75
 1 size_l                0         1     43.4  174.      1     18.9  20.8  23.7 
 2 og                    0         1      1.40   2.18    1      1.05  1.06  1.07
 3 fg                    0         1      1.08   0.432  -0.003  1.01  1.01  1.02
 4 abv                   0         1      6.14   1.90    0      5.08  5.79  6.83
 5 ibu                   0         1     44.3   43.3     0     23.4  35.8  56.4 
 6 color                 0         1     13.4   11.9     0      5.17  8.44 16.8 
 7 boilsize              0         1     49.2  186.      1     20.8  27.4  30   
 8 boiltime              0         1     65.1   15.1     0     60    60    60   
 9 boilgravity        2246         0.959  1.35   1.92    0      1.04  1.05  1.06
10 efficiency            0         1     66.3   14.1     0     65    70    75   
11 primarytemp       16963         0.694 19.2    4.21  -17.8   18    20    20   
     p100 hist 
 1 7800   ▇▁▁▁▁
 2   32.5 ▇▁▁▁▁
 3   23.4 ▇▁▁▁▁
 4   54.7 ▇▁▁▁▁
 5 3409.  ▇▁▁▁▁
 6  186   ▇▁▁▁▁
 7 7800   ▇▁▁▁▁
 8  240   ▁▇▁▁▁
 9   52.6 ▇▁▁▁▁
10  100   ▁▂▁▇▁
11  114   ▁▇▁▁▁
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool, now let's start diving a little deeper into our remaining variables.&lt;/p&gt;
&lt;h2 id="dependentvariables"&gt;Dependent Variables&lt;/h2&gt;
&lt;p&gt;Let's take a look at our dependent variable, style, that we intend on predicting.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Missing data: while its possible that the values could be missing at random (MAR), maybe its possible that when the recipe was input by the user, the style classification that they intended on using was not available so they left it blank, which is intentional and is the assumption I'm going to use meaning that missing values should be their own classification&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the profile above we can see that there are 100+ styles available for us to classify, and the frequency of the top 4 starts off at ~9,000 and quickly drops to the next frequent of 5,600 then to the 2,000s and so on. That's almost 1/3 of the data grouped in the top 4 styles. We could group everything else into an &amp;quot;other&amp;quot; category, so let's choose a threshold that if a style occurs less than 1500 times in our data, that it get's classified as such.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_mutate(style = fct_explicit_na(style)) %&amp;gt;% # turn NA values into their own classification
    step_other(style, threshold = 1500) # group classes appearing less than 2000 times into an &amp;quot;other&amp;quot; group
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec
beer_rec %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% count(style) # count the frequency of remaining styles
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid
Delete terms one_of, vars_remove
Variable mutation for  style
Collapsing factor levels for style
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;A tibble: 6 × 2&lt;/caption&gt;
&lt;thead&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;style&lt;/th&gt;&lt;th scope="col"&gt;n&lt;/th&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;&amp;lt;fct&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;int&amp;gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
	&lt;tr&gt;&lt;td&gt;American Amber Ale  &lt;/td&gt;&lt;td&gt; 1532&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;American IPA        &lt;/td&gt;&lt;td&gt; 8968&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;American Light Lager&lt;/td&gt;&lt;td&gt; 1678&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;American Pale Ale   &lt;/td&gt;&lt;td&gt; 5677&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;Saison              &lt;/td&gt;&lt;td&gt; 1953&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;other               &lt;/td&gt;&lt;td&gt;35588&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Nice, now the problem is that because we have so many styles with less than 1500 entries, all of them pooled together is still an overwhelimg amount. This causes a class imbalance, so essentially our model (depending on the one used) could become bias to choosing &lt;strong&gt;other&lt;/strong&gt; since it's learned on more data that is classified as &lt;strong&gt;other&lt;/strong&gt; compared to the alternate available labels. One trick to accomodate for this is to downsample our data so that we end up with relatively similar amounts of rows for each style, i.e giving all classes an equal opportunity to be learned by the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_downsample(style)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec
beer_rec  %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% count(style)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid
Delete terms one_of, vars_remove
Variable mutation for  style
Collapsing factor levels for style
Down-sampling based on style
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;A tibble: 6 × 2&lt;/caption&gt;
&lt;thead&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;style&lt;/th&gt;&lt;th scope="col"&gt;n&lt;/th&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;&amp;lt;fct&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;int&amp;gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
	&lt;tr&gt;&lt;td&gt;American Amber Ale  &lt;/td&gt;&lt;td&gt;1532&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;American IPA        &lt;/td&gt;&lt;td&gt;1532&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;American Light Lager&lt;/td&gt;&lt;td&gt;1532&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;American Pale Ale   &lt;/td&gt;&lt;td&gt;1532&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;Saison              &lt;/td&gt;&lt;td&gt;1532&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;other               &lt;/td&gt;&lt;td&gt;1532&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even Steven!&lt;/p&gt;
&lt;h2 id="independentvariables"&gt;Independent Variables&lt;/h2&gt;
&lt;p&gt;Looking at our most recent profiling (skim) of the data above, we still have some missing data for both boilgravity and primarytemp. These measurements seem to actually be missing at random; both appear to be measurements that are taken at particular times in the brewing process and could have easily been missed. With this info, we know we will have to impute these variables&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_bagimpute(boilgravity, primarytemp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let's take a look at the distributions of our variables, as well as what they look like against each other.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec %&amp;gt;%
    prep() %&amp;gt;%
    juice() %&amp;gt;%
    select(-style) %&amp;gt;%
    ggpairs()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2020/03/output_33_1.png" alt="Beer Me!"&gt;&lt;/p&gt;
&lt;p&gt;I know this plot is a little bit messy, but just to point out a few things I notice... Looking at the variables individually, there appears to be some skew amongst all of them, so we can adjust this with a transformation like BoxCox or YeoJohnson.&lt;/p&gt;
&lt;p&gt;Regarding the categorical vars, sugarscale is heavily imbalanced.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I learned that brewers can define the specific gravity of the beer, but some like to use what is known as the &amp;quot;Plato&amp;quot; scale. Since we are here, lets take this opportunity to convert gravity measurements with Plato values to Specifc Gravity. SG = (4P/1000) + 1&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_YeoJohnson(all_numeric()) %&amp;gt;% # remove some skew from skewed vars
    step_mutate_at(fg, og, boilgravity, fn = ~((4 * .)/1000) + 1) %&amp;gt;% # convert final gravity using plato values to specific gravity
    step_rm(sugarscale) # remove sugarscale column because now specific gravity is the only value    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec
beer_rec %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% glimpse()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid
Delete terms one_of, vars_remove
Variable mutation for  style
Collapsing factor levels for style
Down-sampling based on style
Bagged tree imputation for boilgravity, primarytemp
Yeo-Johnson transformation on all_numeric
Variable mutation for fg, og, boilgravity
Delete terms sugarscale


Observations: 9,192
Variables: 13
$ style       &amp;lt;fct&amp;gt; American Amber Ale, American Amber Ale, American Amber Al…
$ size_l      &amp;lt;dbl&amp;gt; 1.775784, 1.803888, 1.803888, 1.886012, 1.855547, 1.82880…
$ og          &amp;lt;dbl&amp;gt; 1.004300, 1.004240, 1.004240, 1.004220, 1.004260, 1.00426…
$ fg          &amp;lt;dbl&amp;gt; 1.004036, 1.004064, 1.004068, 1.004048, 1.004060, 1.00406…
$ abv         &amp;lt;dbl&amp;gt; 2.850152, 2.453214, 2.433714, 2.479746, 2.661498, 2.66591…
$ ibu         &amp;lt;dbl&amp;gt; 0.000000, 5.781126, 7.438330, 9.561051, 11.584073, 10.097…
$ color       &amp;lt;dbl&amp;gt; 1.669649, 2.202502, 1.691027, 2.022844, 1.962021, 1.98490…
$ boilsize    &amp;lt;dbl&amp;gt; 1.898764, 1.955817, 1.661273, 1.961717, 1.971877, 2.00135…
$ boiltime    &amp;lt;dbl&amp;gt; 72.25711, 49.32056, 41.53138, 25.63932, 72.25711, 49.3205…
$ boilgravity &amp;lt;dbl&amp;gt; 1.004210, 1.004180, 1.004210, 1.004220, 1.004224, 1.00418…
$ efficiency  &amp;lt;dbl&amp;gt; 178237.01, 220129.90, 13632.68, 220129.90, 211268.70, 194…
$ brewmethod  &amp;lt;fct&amp;gt; All Grain, All Grain, extract, BIAB, All Grain, All Grain…
$ primarytemp &amp;lt;dbl&amp;gt; 18.67447, 19.22292, 18.68587, 17.32834, 17.32834, 18.6628…
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have one categorical independent variable left, brewmethod. Depending on the type of modelling technique we use, this variable would have to be dummy/one-hot encoded. A model like KNN would require this, but a random forest would not. It shouldn't hurt though, and since I intend on trying KNN, lets one-hot encode it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_dummy(all_nominal(), -style, one_hot = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% glimpse()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 9,192
Variables: 16
$ style                   &amp;lt;fct&amp;gt; American Amber Ale, American Amber Ale, Ameri…
$ size_l                  &amp;lt;dbl&amp;gt; 1.775784, 1.803888, 1.803888, 1.886012, 1.855…
$ og                      &amp;lt;dbl&amp;gt; 1.004300, 1.004240, 1.004240, 1.004220, 1.004…
$ fg                      &amp;lt;dbl&amp;gt; 1.004036, 1.004064, 1.004068, 1.004048, 1.004…
$ abv                     &amp;lt;dbl&amp;gt; 2.850152, 2.453214, 2.433714, 2.479746, 2.661…
$ ibu                     &amp;lt;dbl&amp;gt; 0.000000, 5.781126, 7.438330, 9.561051, 11.58…
$ color                   &amp;lt;dbl&amp;gt; 1.669649, 2.202502, 1.691027, 2.022844, 1.962…
$ boilsize                &amp;lt;dbl&amp;gt; 1.898764, 1.955817, 1.661273, 1.961717, 1.971…
$ boiltime                &amp;lt;dbl&amp;gt; 72.25711, 49.32056, 41.53138, 25.63932, 72.25…
$ boilgravity             &amp;lt;dbl&amp;gt; 1.004210, 1.004180, 1.004210, 1.004220, 1.004…
$ efficiency              &amp;lt;dbl&amp;gt; 178237.01, 220129.90, 13632.68, 220129.90, 21…
$ primarytemp             &amp;lt;dbl&amp;gt; 18.67447, 19.22292, 18.68587, 17.32834, 17.32…
$ brewmethod_All.Grain    &amp;lt;dbl&amp;gt; 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, …
$ brewmethod_BIAB         &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, …
$ brewmethod_extract      &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
$ brewmethod_Partial.Mash &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coming into some of the final pre-processing steps we can utilize, lets see if any of our remaining independent variables have any &lt;strong&gt;Near-Zero Variance&lt;/strong&gt;. These sparse &amp;amp; highly imbalanced IVs have low predictive power due to their lack of variation/occurrence relative to the different styles that occur, so let's filter them out.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_nzv(all_numeric())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% glimpse()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 9,192
Variables: 15
$ style                &amp;lt;fct&amp;gt; American Amber Ale, American Amber Ale, American…
$ size_l               &amp;lt;dbl&amp;gt; 1.775784, 1.803888, 1.803888, 1.886012, 1.855547…
$ og                   &amp;lt;dbl&amp;gt; 1.004300, 1.004240, 1.004240, 1.004220, 1.004260…
$ fg                   &amp;lt;dbl&amp;gt; 1.004036, 1.004064, 1.004068, 1.004048, 1.004060…
$ abv                  &amp;lt;dbl&amp;gt; 2.850152, 2.453214, 2.433714, 2.479746, 2.661498…
$ ibu                  &amp;lt;dbl&amp;gt; 0.000000, 5.781126, 7.438330, 9.561051, 11.58407…
$ color                &amp;lt;dbl&amp;gt; 1.669649, 2.202502, 1.691027, 2.022844, 1.962021…
$ boilsize             &amp;lt;dbl&amp;gt; 1.898764, 1.955817, 1.661273, 1.961717, 1.971877…
$ boiltime             &amp;lt;dbl&amp;gt; 72.25711, 49.32056, 41.53138, 25.63932, 72.25711…
$ boilgravity          &amp;lt;dbl&amp;gt; 1.004210, 1.004180, 1.004210, 1.004220, 1.004224…
$ efficiency           &amp;lt;dbl&amp;gt; 178237.01, 220129.90, 13632.68, 220129.90, 21126…
$ primarytemp          &amp;lt;dbl&amp;gt; 18.67447, 19.22292, 18.68587, 17.32834, 17.32834…
$ brewmethod_All.Grain &amp;lt;dbl&amp;gt; 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, …
$ brewmethod_BIAB      &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, …
$ brewmethod_extract   &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next lets see if any issues of high correlation are present.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec %&amp;gt;%
    prep() %&amp;gt;%
    juice() %&amp;gt;%
    select_if(is.numeric) %&amp;gt;%
    select_at(vars(-starts_with(&amp;quot;brewmethod&amp;quot;))) %&amp;gt;%
    ggcorr(nbreaks = 5, label = TRUE) # look to see potential multicollinearity
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2020/03/output_44_0.png" alt="Beer Me!"&gt;&lt;/p&gt;
&lt;p&gt;Clearly we have some high correlations amongst a few variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;boilgravity vs fg&lt;/li&gt;
&lt;li&gt;boilgravity vs og&lt;/li&gt;
&lt;li&gt;boilsize vs size_l&lt;/li&gt;
&lt;li&gt;fg vs og&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now we can implement a filter to remove highly correlated variables, which would imply that the variables may be redundant of each other. Additionally, after removing highly correlated variables, let's also remove any variables that may be linear combinations of each other, implying a potential issue with multicollinearity.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_corr(all_numeric(), threshold = 0.8) %&amp;gt;%
    step_lincomb(all_numeric())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, because several of our continuous numerical variables live within different scales of each other, let's normalize them all to bring them within the same space.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec = beer_rec %&amp;gt;%
    step_normalize(all_numeric(), -starts_with(&amp;quot;brewmethod&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec %&amp;gt;% prep() %&amp;gt;% juice() %&amp;gt;% glimpse()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 9,192
Variables: 12
$ style                &amp;lt;fct&amp;gt; American Amber Ale, American Amber Ale, American…
$ size_l               &amp;lt;dbl&amp;gt; -0.20380362, -0.05002251, -0.05002251, 0.3993566…
$ fg                   &amp;lt;dbl&amp;gt; -0.1531746, -0.1348155, -0.1321928, -0.1453064, …
$ abv                  &amp;lt;dbl&amp;gt; 1.02733267, -0.06087288, -0.11433241, 0.01186519…
$ ibu                  &amp;lt;dbl&amp;gt; -2.27735546, -0.73114821, -0.28791631, 0.2798221…
$ color                &amp;lt;dbl&amp;gt; 0.105442916, 1.797271835, 0.173318912, 1.2268501…
$ boiltime             &amp;lt;dbl&amp;gt; 2.000994, -0.296342, -1.076510, -2.668266, 2.000…
$ efficiency           &amp;lt;dbl&amp;gt; 0.1164106, 0.6481222, -1.9727751, 0.6481222, 0.5…
$ primarytemp          &amp;lt;dbl&amp;gt; -0.12042289, 0.05232143, -0.11683074, -0.5444035…
$ brewmethod_All.Grain &amp;lt;dbl&amp;gt; 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, …
$ brewmethod_BIAB      &amp;lt;dbl&amp;gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, …
$ brewmethod_extract   &amp;lt;dbl&amp;gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's get one last look at our final recipe.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;beer_rec %&amp;gt;% prep()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data Recipe

Inputs:

  23 variables (no declared roles)

Training data contained 55396 data points and 37817 incomplete rows. 

Operations:

Variable renaming for BeerID, Name, URL, Style, StyleID, ... [trained]
Variable renaming for  size_l [trained]
Variable mutation for name, url, style, ... [trained]
Variable mutation for boilgravity, primarytemp [trained]
Variables removed beerid, name, url, styleid, userid [trained]
Variables removed mashthickness, pitchrate, primingmethod, primingamount [trained]
Variable mutation for  style [trained]
Collapsing factor levels for style [trained]
Down-sampling based on style [trained]
Bagged tree imputation for boilgravity, primarytemp [trained]
Yeo-Johnson transformation on size_l, abv, ibu, color, ... [trained]
Variable mutation for fg, og, boilgravity [trained]
Variables removed sugarscale [trained]
Dummy variables from brewmethod [trained]
Sparse, unbalanced variable filter removed brewmethod_Partial.Mash [trained]
Correlation filter removed boilsize, boilgravity, og [trained]
Linear combination filter removed no terms [trained]
Centering and scaling for size_l, fg, abv, ibu, color, ... [trained]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="modeling"&gt;Modeling&lt;/h1&gt;
&lt;p&gt;Let's bake our recipe into our test data. Remember, our test data hasn't seen any of these steps since we are treating it as brand new data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;test_data = beer_rec %&amp;gt;%
    prep() %&amp;gt;%
    bake(test)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's try out several different models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K Nearest Neighbours (KNN)&lt;/li&gt;
&lt;li&gt;Decision Tree&lt;/li&gt;
&lt;li&gt;Random Forest&lt;/li&gt;
&lt;li&gt;XGBoost&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;train_data = beer_rec %&amp;gt;% prep() %&amp;gt;% juice()

set.seed(0415)

knn = nearest_neighbor() %&amp;gt;%
    set_engine(&amp;quot;kknn&amp;quot;) %&amp;gt;%
    set_mode(&amp;quot;classification&amp;quot;)

knn_fit = knn %&amp;gt;%
    fit(style ~ ., train_data)

set.seed(0415)

dectree = decision_tree() %&amp;gt;%
    set_engine(&amp;quot;rpart&amp;quot;) %&amp;gt;%
    set_mode(&amp;quot;classification&amp;quot;)

dectree_fit = dectree %&amp;gt;%
    fit(style ~ ., train_data)

# Ensembles
set.seed(0415)

rf = rand_forest() %&amp;gt;%
    set_engine(&amp;quot;randomForest&amp;quot;) %&amp;gt;%
    set_mode(&amp;quot;classification&amp;quot;)

rf_fit = rf %&amp;gt;%
    fit(style ~ ., train_data)

set.seed(0415)

xgb = boost_tree() %&amp;gt;%
    set_engine(&amp;quot;xgboost&amp;quot;) %&amp;gt;%
    set_mode(&amp;quot;classification&amp;quot;)

xgb_fit = xgb %&amp;gt;%
    fit(style ~ ., train_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;knn_fit
dectree_fit
rf_fit
xgb_fit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;parsnip model object

Fit time:  3.3s 

Call:
kknn::train.kknn(formula = formula, data = data, ks = 5)

Type of response variable: nominal
Minimal misclassification: 0.5039164
Best kernel: optimal
Best k: 5



parsnip model object

Fit time:  533ms 
n= 9192 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 9192 7660 American Amber Ale (0.17 0.17 0.17 0.17 0.17 0.17)  
   2) primarytemp&amp;lt; 0.7528845 8089 6593 American IPA (0.18 0.18 0.18 0.18 0.083 0.18)  
     4) color&amp;gt;=0.4493611 2859 1625 American Amber Ale (0.43 0.084 0.14 0.053 0.021 0.27)  
       8) color&amp;lt; 1.398371 2125  961 American Amber Ale (0.55 0.095 0.096 0.066 0.021 0.17) *
       9) color&amp;gt;=1.398371 734  327 other (0.095 0.054 0.26 0.014 0.022 0.55) *
     5) color&amp;lt; 0.4493611 5230 3891 American Pale Ale (0.048 0.24 0.21 0.26 0.12 0.13)  
      10) ibu&amp;gt;=0.5376232 1459  569 American IPA (0.027 0.61 0.12 0.15 0.019 0.071) *
      11) ibu&amp;lt; 0.5376232 3771 2658 American Pale Ale (0.056 0.097 0.24 0.3 0.15 0.16)  
        22) ibu&amp;lt; -0.612908 932  426 American Light Lager (0.033 0.05 0.54 0.064 0.068 0.24) *
        23) ibu&amp;gt;=-0.612908 2839 1786 American Pale Ale (0.064 0.11 0.14 0.37 0.18 0.13)  
          46) primarytemp&amp;lt; 0.3408113 2305 1367 American Pale Ale (0.067 0.12 0.16 0.41 0.1 0.15) *
          47) primarytemp&amp;gt;=0.3408113 534  247 Saison (0.051 0.082 0.051 0.22 0.54 0.064) *
   3) primarytemp&amp;gt;=0.7528845 1103  244 Saison (0.042 0.033 0.056 0.038 0.78 0.053) *



parsnip model object

Fit time:  17.2s 

Call:
 randomForest(x = as.data.frame(x), y = y) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 3

        OOB estimate of  error rate: 30.36%
Confusion matrix:
                     American Amber Ale American IPA American Light Lager
American Amber Ale                 1151           83                   23
American IPA                         97         1073                   29
American Light Lager                 64           91                 1040
American Pale Ale                   138          242                   49
Saison                               48           55                   25
other                               166           94                  123
                     American Pale Ale Saison other class.error
American Amber Ale                 123     48   104   0.2486945
American IPA                       213     56    64   0.2996084
American Light Lager                96     90   151   0.3211488
American Pale Ale                  935    107    61   0.3896867
Saison                              87   1254    63   0.1814621
other                              107     94   948   0.3812010



parsnip model object

Fit time:  1.6s 
##### xgb.Booster
raw: 295.8 Kb 
call:
  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, 
    colsample_bytree = 1, min_child_weight = 1, subsample = 1), 
    data = x, nrounds = 15, verbose = 0, objective = &amp;quot;multi:softprob&amp;quot;, 
    num_class = 6L, nthread = 1)
params (as set within xgb.train):
  eta = &amp;quot;0.3&amp;quot;, max_depth = &amp;quot;6&amp;quot;, gamma = &amp;quot;0&amp;quot;, colsample_bytree = &amp;quot;1&amp;quot;, min_child_weight = &amp;quot;1&amp;quot;, subsample = &amp;quot;1&amp;quot;, objective = &amp;quot;multi:softprob&amp;quot;, num_class = &amp;quot;6&amp;quot;, nthread = &amp;quot;1&amp;quot;, silent = &amp;quot;1&amp;quot;
xgb.attributes:
  niter
# of features: 11 
niter: 15
nfeatures : 11 
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="evaluatingmodel"&gt;Evaluating model&lt;/h1&gt;
&lt;p&gt;So we've trained our models. However, we can potentially improve their predictabilities using &lt;strong&gt;Cross-Fold Validation&lt;/strong&gt;. The idea here is to give the models different looks of the data by creating multiple data sets that are essentially different subsets of the original training data. We then train the data on all of these new sets, evaluate the performance and select the model based on a particular criteria.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;folds = vfold_cv(train_data, strata = style)
glimpse(folds)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Observations: 10
Variables: 2
$ splits &amp;lt;named list&amp;gt; [&amp;lt;rsplit[8268 x 924 x 9192 x 12]&amp;gt;, &amp;lt;rsplit[8268 x 924 x…
$ id     &amp;lt;chr&amp;gt; &amp;quot;Fold01&amp;quot;, &amp;quot;Fold02&amp;quot;, &amp;quot;Fold03&amp;quot;, &amp;quot;Fold04&amp;quot;, &amp;quot;Fold05&amp;quot;, &amp;quot;Fold06&amp;quot;, &amp;quot;F…
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;knn_res = fit_resamples(
    style ~ .,
    knn,
    folds,
    control = control_resamples(save_pred = TRUE)
)

dectree_res = fit_resamples(
    style ~ .,
    dectree,
    folds,
    control = control_resamples(save_pred = TRUE)
)

rf_res = fit_resamples(
    style ~ .,
    rf,
    folds,
    control = control_resamples(save_pred = TRUE)
)

xgb_res = fit_resamples(
    style ~ .,
    xgb,
    folds,
    control = control_resamples(save_pred = TRUE)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;knn_res %&amp;gt;%
    collect_metrics() %&amp;gt;%
    mutate(model = &amp;quot;knn&amp;quot;) %&amp;gt;%
    bind_rows(
        dectree_res %&amp;gt;%
            collect_metrics() %&amp;gt;%
            mutate(model = &amp;quot;dectree&amp;quot;)
    ) %&amp;gt;%
    bind_rows(
        rf_res %&amp;gt;%
            collect_metrics() %&amp;gt;%
            mutate(model = &amp;quot;rf&amp;quot;)
    ) %&amp;gt;%
    bind_rows(
        xgb_res %&amp;gt;%
            collect_metrics() %&amp;gt;%
            mutate(model = &amp;quot;xgb&amp;quot;)
    ) %&amp;gt;%
    arrange(.metric, desc(mean))
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;A tibble: 8 × 6&lt;/caption&gt;
&lt;thead&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;.metric&lt;/th&gt;&lt;th scope="col"&gt;.estimator&lt;/th&gt;&lt;th scope="col"&gt;mean&lt;/th&gt;&lt;th scope="col"&gt;n&lt;/th&gt;&lt;th scope="col"&gt;std_err&lt;/th&gt;&lt;th scope="col"&gt;model&lt;/th&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;&amp;lt;chr&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;chr&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;dbl&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;int&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;dbl&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;chr&amp;gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
	&lt;tr&gt;&lt;td&gt;accuracy&lt;/td&gt;&lt;td&gt;multiclass&lt;/td&gt;&lt;td&gt;0.7089834&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;0.005538325&lt;/td&gt;&lt;td&gt;xgb    &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;accuracy&lt;/td&gt;&lt;td&gt;multiclass&lt;/td&gt;&lt;td&gt;0.6969004&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;0.005536357&lt;/td&gt;&lt;td&gt;rf     &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;accuracy&lt;/td&gt;&lt;td&gt;multiclass&lt;/td&gt;&lt;td&gt;0.5440306&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;0.006265448&lt;/td&gt;&lt;td&gt;dectree&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;accuracy&lt;/td&gt;&lt;td&gt;multiclass&lt;/td&gt;&lt;td&gt;0.4919383&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;0.003866652&lt;/td&gt;&lt;td&gt;knn    &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;roc_auc &lt;/td&gt;&lt;td&gt;hand_till &lt;/td&gt;&lt;td&gt;0.9262713&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;0.002184205&lt;/td&gt;&lt;td&gt;xgb    &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;roc_auc &lt;/td&gt;&lt;td&gt;hand_till &lt;/td&gt;&lt;td&gt;0.9140980&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;0.002991242&lt;/td&gt;&lt;td&gt;rf     &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;roc_auc &lt;/td&gt;&lt;td&gt;hand_till &lt;/td&gt;&lt;td&gt;0.7923928&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;0.003745520&lt;/td&gt;&lt;td&gt;knn    &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;roc_auc &lt;/td&gt;&lt;td&gt;hand_till &lt;/td&gt;&lt;td&gt;0.7904161&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;0.004375007&lt;/td&gt;&lt;td&gt;dectree&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that XGBoost outperformed the others, barely beating out random forest. Let's visualize the ROC curve of all.&lt;/p&gt;
&lt;p&gt;Also, let's see the confusion matrix for our winner.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;knn_res %&amp;gt;%
    unnest(.predictions) %&amp;gt;%
    mutate(model = &amp;quot;knn&amp;quot;) %&amp;gt;%
    bind_rows(
        dectree_res %&amp;gt;%
            unnest(.predictions) %&amp;gt;%
            mutate(model = &amp;quot;dectree&amp;quot;)
    ) %&amp;gt;%
    bind_rows(
        rf_res %&amp;gt;%
            unnest(.predictions) %&amp;gt;%
            mutate(model = &amp;quot;rf&amp;quot;)
    ) %&amp;gt;%
    bind_rows(
        xgb_res %&amp;gt;%
            unnest(.predictions) %&amp;gt;%
            mutate(model = &amp;quot;xgb&amp;quot;)
    ) %&amp;gt;%
    group_by(model) %&amp;gt;%
    roc_curve(starts_with(&amp;quot;.pred_&amp;quot;), -.pred_class, truth = style) %&amp;gt;%
    autoplot()

# Confusion Matrix for XGBoost
xgb_res %&amp;gt;%
    unnest(.predictions) %&amp;gt;%
    conf_mat(style, .pred_class) %&amp;gt;%
    autoplot(type = &amp;quot;heatmap&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2020/03/output_63_0.png" alt="Beer Me!"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2020/03/output_63_1.png" alt="Beer Me!"&gt;&lt;/p&gt;
&lt;p&gt;Now, we will test XGBoost against our &lt;strong&gt;test&lt;/strong&gt; data to see how well it performs on unseen data!&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-R"&gt;class_metrics = metric_set(accuracy)

xgb_fit %&amp;gt;%
    predict(new_data = test_data) %&amp;gt;%
    class_metrics(truth = test_data$style, estimate = .pred_class)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;A tibble: 1 × 3&lt;/caption&gt;
&lt;thead&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;.metric&lt;/th&gt;&lt;th scope="col"&gt;.estimator&lt;/th&gt;&lt;th scope="col"&gt;.estimate&lt;/th&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;&amp;lt;chr&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;chr&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;dbl&amp;gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
	&lt;tr&gt;&lt;td&gt;accuracy&lt;/td&gt;&lt;td&gt;multiclass&lt;/td&gt;&lt;td&gt;0.6631465&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;66% of the new styles were correctly classified. That's ok. Not as good as we performed on the training data, but not too far away either.&lt;/p&gt;
&lt;p&gt;Some possible areas of improvement:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Investigating if there is any value that could be pulled out of the name of the beer.&lt;/li&gt;
&lt;li&gt;Adjusting the &lt;strong&gt;missing data threshold&lt;/strong&gt; for removing variables.&lt;/li&gt;
&lt;li&gt;Analyzing the imputation methods deeper.&lt;/li&gt;
&lt;li&gt;Adjusting the style groupings to include more/less classes&lt;/li&gt;
&lt;li&gt;Hyperparameter Tuning?&lt;/li&gt;
&lt;li&gt;Trying out other models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are many other things that could potentially be improved, but these are just to note a few. Maybe I have some new blog posts now :).&lt;/p&gt;
&lt;p&gt;Thanks.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;</content:encoded></item><item><title>Detecting Power Usage</title><description>In this post, I had some time series data that was measuring electrical outlet usage. The goal was to detect periods of consumption.</description><link>http://localhost:2368/detecting-power-usage/</link><guid isPermaLink="false">5e572ecf63bbcf0c0ec7ab7a</guid><category>Time Series</category><category>Machine Learning</category><category>R</category><dc:creator>Gary Clark Jr.</dc:creator><pubDate>Thu, 27 Feb 2020 03:35:28 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1534224039826-c7a0eda0e6b3?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" medium="image"/><content:encoded>&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="problemstatement"&gt;Problem Statement&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;img src="https://images.unsplash.com/photo-1534224039826-c7a0eda0e6b3?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Detecting Power Usage"&gt;&lt;p&gt;Detect episodes of usage within the sample of data provided, with start and stop timestamps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Show the original data with the usage episodes marked for easy comparison.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Documentation regarding algorithms, data prep steps, and transformations to solve 1 &amp;amp; 2. Describe any problems encountered or stumbling blocks in this kind of analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;rm(list=ls())
options(warn=-1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="loadlibraries"&gt;Load libraries&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;library(dplyr) # data manipulation
library(tidyr) # reshaping data
library(ggplot2) # visualizations
library(gridExtra) # arranging visualizations
library(lubridate) # working with date-time
library(readr) # reading/writing data
library(zoo) # working with time-series data
library(forecast) # useful time-series analytics
library(infer) # applying statistical inference
library(factoextra) # easily applying kmeans elbow method plotting
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="readandviewthedata"&gt;Read and View the Data&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;df = readr::read_csv(&amp;quot;data-sample.csv&amp;quot;)

head(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;A tibble: 6 × 2&lt;/caption&gt;
&lt;thead&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;timestamp&lt;/th&gt;&lt;th scope="col"&gt;power&lt;/th&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;&amp;lt;chr&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;dbl&amp;gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:00:00 EDT&lt;/td&gt;&lt;td&gt;27.41&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:01:00 EDT&lt;/td&gt;&lt;td&gt;27.40&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:02:00 EDT&lt;/td&gt;&lt;td&gt;27.42&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:03:00 EDT&lt;/td&gt;&lt;td&gt;27.42&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:04:00 EDT&lt;/td&gt;&lt;td&gt;27.42&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:05:00 EDT&lt;/td&gt;&lt;td&gt;27.43&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;One of the first things I noticed is that the timestamps have been read in as a character vector. Let's change that to the appropriate data type.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;df = df %&amp;gt;%
  mutate(timestamp = ymd_hms(timestamp, tz = &amp;quot;US/Eastern&amp;quot;))

head(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;A tibble: 6 × 2&lt;/caption&gt;
&lt;thead&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;timestamp&lt;/th&gt;&lt;th scope="col"&gt;power&lt;/th&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;&amp;lt;dttm&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;dbl&amp;gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:00:00&lt;/td&gt;&lt;td&gt;27.41&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:01:00&lt;/td&gt;&lt;td&gt;27.40&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:02:00&lt;/td&gt;&lt;td&gt;27.42&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:03:00&lt;/td&gt;&lt;td&gt;27.42&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:04:00&lt;/td&gt;&lt;td&gt;27.42&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 00:05:00&lt;/td&gt;&lt;td&gt;27.43&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now that our data is in the right format, let's move onto some EDA (Exploratory Data Analysis).&lt;/p&gt;
&lt;h2 id="exploration"&gt;Exploration&lt;/h2&gt;
&lt;h3 id="visualizingthedata"&gt;Visualizing the Data&lt;/h3&gt;
&lt;p&gt;First we will visualize the data to get a clear picture of what we're working with&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;df %&amp;gt;%
  ggplot(aes(x = timestamp, y = power)) +
  geom_line() +
  ggtitle(&amp;quot;Power Consumption over a 24hr Period&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_9_0.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;As mentioned in the exercise, we can see that the power stabilizes at 3 different levels. These &amp;quot;stable&amp;quot; periods are referred to as idle/background consumption. In addition to idle consumption, there also seem to be 2 other features that appear:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Small spikes/noise of about +/- 1 watt&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This could be valid usage, or could also be heavier noise occurring during idle consumption&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Large spikes varying about 10-12 watts&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;These look like valid usage episodes&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="assumptions"&gt;Assumptions&lt;/h3&gt;
&lt;p&gt;Now that I have had the opportunit to briefly look at the data, I'm going to make some assumptions before moving forward.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The data is a &lt;strong&gt;representative sample&lt;/strong&gt; of the population.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The small spikes are noise in idle consumption.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The large spikes are &lt;strong&gt;valid&lt;/strong&gt; usage episodes and what should be targeted for detection.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="describingthedata"&gt;Describing the Data&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;hist = df %&amp;gt;%
  ggplot(aes(power)) +
  geom_histogram()

dens = df %&amp;gt;%
  ggplot(aes(power)) +
  geom_density()

box = df %&amp;gt;%
  ggplot(aes(x = 1, y = power)) +
  geom_boxplot()

summ = summary(df %&amp;gt;% select(-timestamp))

grid.arrange(hist, box, dens, tableGrob(summ), nrow = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_11_1.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;As expected, the histogram and density plots show the shape of the data where the data clusters at the 3 different levels that the idle consumption bounces to. What is a bit unexpected, though, is how no outliers are showing up in the boxplot. I have a hunch that if we look at the spread of data in hourly windows, we would start to see outliers appear. Let's quickly see.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;# Parse the timestamps
df_broken = df %&amp;gt;%
  mutate_at(vars(timestamp), funs(year, month, day, hour))

df_broken %&amp;gt;%
  ggplot(aes(x = as.factor(hour), y = power)) +
  geom_boxplot() +
  xlab(&amp;quot;Hours of the Day&amp;quot;) +
  ggtitle(&amp;quot;Hourly boxplots&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_13_0.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;Now this definitely helps a bit more. We can clearly see the idle consumption levels, and also varying ranges of outliers. This is particularly useful for testing statistical significance.&lt;/p&gt;
&lt;h3 id="transformingthedata"&gt;Transforming the Data&lt;/h3&gt;
&lt;p&gt;We can see a clear downward trend of the idle consumption (and overall data) over time which could make detection a bit more complex. However, we can transform the data by differencing it to see if this makes the data more stationary and removes the trend&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;df_diff = df %&amp;gt;%
  mutate(power_diff = power - lag(power))

df_diff %&amp;gt;%
  gather(&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;, -timestamp) %&amp;gt;%
  ggplot(aes(x = timestamp, y = value)) +
  geom_line() +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE) +
  facet_grid(key~., scales = &amp;quot;free&amp;quot;) +
  ylab(&amp;quot;watts&amp;quot;) +
  ggtitle(&amp;quot;Power vs Power Differencing&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_15_0.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;Power differenced is much more stable, and will prove much easier to work with. Let's re-do the histogram.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;p1 = df_diff %&amp;gt;%
  ggplot(aes(power_diff)) +
  geom_histogram() +
  xlab(&amp;quot;power diff&amp;quot;)

p2 = df_diff %&amp;gt;%
  ggplot(aes(power_diff)) +
  geom_histogram() +
  xlim(c(-0.1,0.1)) +
  xlab(&amp;quot;power diff&amp;quot;) +
  ggtitle(&amp;quot;Zoomed-In&amp;quot;)
  

grid.arrange(p1, p2, nrow = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_17_1.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;p&gt;We can see the differenced background noise consumption (tall peak) is pretty symmetrical (shown better in the plot to the right) and that it fluctuates within about +/- 0.25 of its previous value at times.&lt;/p&gt;
&lt;p&gt;This is a &lt;strong&gt;VERY&lt;/strong&gt; Leptokurtic distribution. It's almost uniform. While this could pose some challenges for different statistical techniques, we will carry forward nonetheless to see if that is the case as the data is much more &amp;quot;normal-like&amp;quot; than prior to differencing.&lt;/p&gt;
&lt;p&gt;Now that our data is more &amp;quot;normal-like&amp;quot;, we can attempt to describe it with some stats. Namely the 2 that would describe it best are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The mean for its central tendency&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The standard deviation for its spread&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Instead of getting these values straight-up, we can potentially get more value out of how they change over time. To do this I'll use an hour long window that will roll every minute over the &lt;strong&gt;Power Difference&lt;/strong&gt; collecting the &lt;em&gt;mean&lt;/em&gt; and &lt;em&gt;standard deviation&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;At this point I am also going to transform the Power Difference by taking the absolute value of it. If we refer back to the 3rd assumption, that we are to detect the large spikes, then if a large spike up would denote the start of a usage period, a large spike down would mean the end of the period. For this analysis, I'm only going to be concerned with the magnitude of the change. (It's possible that direction could be useful if used correctly)&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;# Convert power difference to absolute value
df_diff$power_diff = abs(df_diff$power_diff)

# Convert data to time-series object
df_diff_ts = zoo(df_diff$power_diff, order.by = df_diff$timestamp)

# Calculate rollilng stats
df_diff$mean_diff = rollapply(df_diff_ts, width = 60, FUN = mean, fill = NA)
df_diff$sd_diff = rollapply(df_diff_ts, width = 60, FUN = sd, fill = NA)

# Renaming the variable names for ordering of plot and turning the mean_diff and sd_diff back to numeric data types
df_diff = df_diff %&amp;gt;%
  mutate_at(vars(mean_diff, sd_diff), as.numeric)
  

df_diff %&amp;gt;%
  rename(
    &amp;quot;a_power&amp;quot;=power,
    &amp;quot;b_power_diff&amp;quot;=power_diff,
    &amp;quot;c_mean_diff&amp;quot;=mean_diff,
    &amp;quot;d_sd_diff&amp;quot;=sd_diff
  ) %&amp;gt;%
  gather(&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;, -timestamp) %&amp;gt;%
  ggplot(aes(x = timestamp, y = value)) +
  geom_line() +
  facet_grid(key~., scales = &amp;quot;free&amp;quot;) +
  ylab(&amp;quot;watts&amp;quot;) +
  ggtitle(&amp;quot;Rolling Mean &amp;amp; Std Dev on Power Difference&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_19_0.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="detectingusage"&gt;Detecting Usage&lt;/h2&gt;
&lt;p&gt;Now that we have several different views of our data, let's look to see if we can use them to detect when usage occurs. I think setting thresholds is a logical &amp;amp; simple first step to seeing if it can get the job done.&lt;/p&gt;
&lt;h3 id="settingthresholdsusinginferentialstatistics"&gt;Setting Thresholds Using Inferential Statistics&lt;/h3&gt;
&lt;p&gt;Going back to our assumption #1 that the data is a &lt;strong&gt;representative sample&lt;/strong&gt;, we can use it to infer what the mean and standard deviation of the population could be. This is performed by repeatedly sampling the data &lt;em&gt;with replacement&lt;/em&gt;, calculating a stat across each bootsrap, and forming the &lt;em&gt;Null Distribution&lt;/em&gt;. From there we can put confidence intervals around and use the intervals as our thresholds.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;# Get overall stats of absolute power difference
overall_stats = df_diff %&amp;gt;%
  summarise(mean = mean(power_diff, na.rm = TRUE),
            sd = sd(power_diff, na.rm = TRUE))

mean_diff_overall = overall_stats %&amp;gt;% pull(mean)
sd_diff_overall = overall_stats %&amp;gt;% pull(sd)

# Create null distributions
mean_diff_null_distn = df_diff %&amp;gt;%
  specify(response = power_diff) %&amp;gt;%
  hypothesize(null = &amp;quot;point&amp;quot;, mu = mean_diff_overall) %&amp;gt;%
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;) %&amp;gt;%
  calculate(stat = &amp;quot;mean&amp;quot;)

sd_diff_null_distn = df_diff %&amp;gt;%
  specify(response = power_diff) %&amp;gt;%
  hypothesize(null = &amp;quot;point&amp;quot;, sigma = sd_diff_overall) %&amp;gt;%
  generate(reps = 1000, type = &amp;quot;bootstrap&amp;quot;) %&amp;gt;%
  calculate(stat = &amp;quot;sd&amp;quot;)

# Get confidence intervals - use 97.5% as we are only concerned with values that are significantly larger
ci_mean = mean_diff_null_distn %&amp;gt;% get_ci() %&amp;gt;% pull(`97.5%`)
ci_sd = sd_diff_null_distn %&amp;gt;% get_ci() %&amp;gt;% pull(`97.5%`)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our thresholds let's apply them and see how we did.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;df_diff_usage = df_diff %&amp;gt;%
  mutate(usage = ifelse((power_diff &amp;gt; 2.5 &amp;amp; mean_diff &amp;gt; ci_mean &amp;amp; sd_diff &amp;gt; ci_sd), TRUE, FALSE))

df_diff_usage %&amp;gt;%
  rename(
    &amp;quot;a_power&amp;quot;=power,
    &amp;quot;b_power_diff&amp;quot;=power_diff,
    &amp;quot;c_mean_diff&amp;quot;=mean_diff,
    &amp;quot;d_sd_diff&amp;quot;=sd_diff
  ) %&amp;gt;%
  gather(&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;, -timestamp, -usage) %&amp;gt;%
  ggplot(aes(x = timestamp, y = value)) +
  geom_vline(xintercept = df_diff_usage$timestamp[which(df_diff_usage$usage)], color = &amp;quot;red&amp;quot;) +
  geom_line() +
  facet_grid(key ~ ., scales = &amp;quot;free&amp;quot;) +
  ylab(&amp;quot;watts&amp;quot;) +
  ggtitle(&amp;quot;Statistical Inference&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_23_0.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code class="language-r"&gt;start = c()
end = c()

df_diff_usage$usage[1] = FALSE

for(i in 2:(nrow(df_diff_usage) - 1)) {
  if(df_diff_usage$usage[i]) {
    if((df_diff_usage$usage[i-1] == TRUE &amp;amp; df_diff_usage$usage[i+1] == TRUE)) {
      next
    } else if ((df_diff_usage$usage[i-1] == FALSE &amp;amp; df_diff_usage$usage[i+1] == TRUE)) {
      start[[i]] = df_diff_usage$timestamp[i]
    } else if ((df_diff_usage$usage[i-1] == TRUE &amp;amp; df_diff_usage$usage[i+1] == FALSE)) {
      end[[i]] = df_diff_usage$timestamp[i]
    } else {
      start[[i]] = df_diff_usage$timestamp[i]
      end[[i]] = df_diff_usage$timestamp[i]
    }
      
  } else {
    next
  }
}

start = as_datetime(start[complete.cases(start)], tz = &amp;quot;US/Eastern&amp;quot;)
end = as_datetime(end[complete.cases(end)], tz = &amp;quot;US/Eastern&amp;quot;)

final_df = data.frame(start = start, end = end) %&amp;gt;%
  mutate(usage_length = end - start,
         usage_length = ifelse(usage_length == 0, &amp;quot;&amp;lt;60 secs&amp;quot;, usage_length))

final_df
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;A data.frame: 18 × 3&lt;/caption&gt;
&lt;thead&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;start&lt;/th&gt;&lt;th scope="col"&gt;end&lt;/th&gt;&lt;th scope="col"&gt;usage_length&lt;/th&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;&amp;lt;dttm&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;dttm&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;chr&amp;gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 09:32:00&lt;/td&gt;&lt;td&gt;2019-06-14 09:33:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 09:35:00&lt;/td&gt;&lt;td&gt;2019-06-14 09:36:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 09:38:00&lt;/td&gt;&lt;td&gt;2019-06-14 09:38:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:00:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:00:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:03:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:03:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:07:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:07:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:10:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:13:00&lt;/td&gt;&lt;td&gt;180     &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:15:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:15:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 13:44:00&lt;/td&gt;&lt;td&gt;2019-06-14 13:45:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 13:49:00&lt;/td&gt;&lt;td&gt;2019-06-14 13:51:00&lt;/td&gt;&lt;td&gt;120     &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 13:55:00&lt;/td&gt;&lt;td&gt;2019-06-14 13:56:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:00:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:00:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:51:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:52:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:54:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:54:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:56:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:57:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:59:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:59:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 16:26:00&lt;/td&gt;&lt;td&gt;2019-06-14 16:26:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 16:29:00&lt;/td&gt;&lt;td&gt;2019-06-14 16:32:00&lt;/td&gt;&lt;td&gt;180     &lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="usingunsupervisedlearning"&gt;Using Unsupervised Learning&lt;/h3&gt;
&lt;p&gt;I feel that the above is a sound method for this exercise, however, I am curious to see how an algorithm like KMeans could perform against this data. I will let it cluster the data against the transformations I created, i.e power_diff, mean_diff, and sd_diff.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;df_km_usage = df_diff %&amp;gt;%
  filter(!is.na(mean_diff),
         !is.na(sd_diff))

set.seed(0415)

# Find optimal number of clusters using the elbow method
fviz_nbclust(df_km_usage %&amp;gt;% select(-timestamp, -power), kmeans, method = &amp;quot;wss&amp;quot;)

real_k = kmeans(df_km_usage %&amp;gt;% select(-timestamp, -power), centers = 3)
df_km_usage$cluster = real_k$cluster

df_km_usage %&amp;gt;%
  rename(
    &amp;quot;a_power&amp;quot;=power,
    &amp;quot;b_power_diff&amp;quot;=power_diff,
    &amp;quot;c_mean_diff&amp;quot;=mean_diff,
    &amp;quot;d_sd_diff&amp;quot;=sd_diff
    # &amp;quot;e_usage&amp;quot;=usage
  ) %&amp;gt;%
  gather(&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;, -timestamp, -cluster) %&amp;gt;%
  ggplot(aes(x=timestamp, y = value)) +
  geom_vline(xintercept = df_km_usage$timestamp[df_km_usage$cluster == 2], col = &amp;quot;red&amp;quot;) +
  geom_line() +
  facet_grid(key~., scales=&amp;quot;free&amp;quot;) +
  ylab(&amp;quot;watts&amp;quot;) +
  ggtitle(&amp;quot;KMeans Usage Detection&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_26_0.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/02/output_26_1.png" class="kg-image" alt="Detecting Power Usage"&gt;&lt;/figure&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;pre&gt;&lt;code class="language-r"&gt;df_km_us = df_km_usage %&amp;gt;%
  mutate(usage = ifelse(cluster == 2, TRUE, FALSE))

start_k = c()
end_k = c()

for(i in 2:(nrow(df_km_us) - 1)) {
  if(df_km_us$usage[i]) {
    
    if((df_km_us$usage[i-1] == TRUE &amp;amp; df_km_us$usage[i+1] == TRUE)) {
      next
    }
    else if ((df_km_us$usage[i-1] == FALSE &amp;amp; df_km_us$usage[i+1] == TRUE)) {
      start_k[[i]] = df_km_us$timestamp[i]
    } else if ((df_km_us$usage[i-1] == TRUE &amp;amp; df_km_us$usage[i+1] == FALSE)) {
      end_k[[i]] = df_km_us$timestamp[i]
    } else {
      start_k[[i]] = df_km_us$timestamp[i]
      end_k[[i]] = df_km_us$timestamp[i]
    }
      
  } else {
    next
  }
}

start_k = as_datetime(start_k[complete.cases(start_k)], tz = &amp;quot;US/Eastern&amp;quot;)
end_k = as_datetime(end_k[complete.cases(end_k)], tz = &amp;quot;US/Eastern&amp;quot;)

final_df_k = data.frame(start_k = start_k, end_k = end_k) %&amp;gt;%
  mutate(usage_length_k = end_k - start_k,
         usage_length_k = ifelse(usage_length_k == 0, &amp;quot;&amp;lt;60 secs&amp;quot;, usage_length_k))

final_df_k
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;A data.frame: 20 × 3&lt;/caption&gt;
&lt;thead&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;start_k&lt;/th&gt;&lt;th scope="col"&gt;end_k&lt;/th&gt;&lt;th scope="col"&gt;usage_length_k&lt;/th&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;th scope="col"&gt;&amp;lt;dttm&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;dttm&amp;gt;&lt;/th&gt;&lt;th scope="col"&gt;&amp;lt;chr&amp;gt;&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 06:02:00&lt;/td&gt;&lt;td&gt;2019-06-14 06:02:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 09:32:00&lt;/td&gt;&lt;td&gt;2019-06-14 09:33:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 09:35:00&lt;/td&gt;&lt;td&gt;2019-06-14 09:36:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 09:38:00&lt;/td&gt;&lt;td&gt;2019-06-14 09:38:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:00:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:00:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:03:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:03:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:07:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:07:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:10:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:12:00&lt;/td&gt;&lt;td&gt;120     &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 10:15:00&lt;/td&gt;&lt;td&gt;2019-06-14 10:15:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 11:54:00&lt;/td&gt;&lt;td&gt;2019-06-14 11:55:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 13:44:00&lt;/td&gt;&lt;td&gt;2019-06-14 13:45:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 13:49:00&lt;/td&gt;&lt;td&gt;2019-06-14 13:51:00&lt;/td&gt;&lt;td&gt;120     &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 13:55:00&lt;/td&gt;&lt;td&gt;2019-06-14 13:56:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:00:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:00:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:51:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:52:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:54:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:54:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:56:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:57:00&lt;/td&gt;&lt;td&gt;60      &lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 14:59:00&lt;/td&gt;&lt;td&gt;2019-06-14 14:59:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 16:26:00&lt;/td&gt;&lt;td&gt;2019-06-14 16:26:00&lt;/td&gt;&lt;td&gt;&amp;lt;60 secs&lt;/td&gt;&lt;/tr&gt;
	&lt;tr&gt;&lt;td&gt;2019-06-14 16:29:00&lt;/td&gt;&lt;td&gt;2019-06-14 16:32:00&lt;/td&gt;&lt;td&gt;180     &lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;h3 id="methodologydecision"&gt;Methodology Decision&lt;/h3&gt;
&lt;p&gt;I think that both methods worked pretty well with the data. I believe there are pros and cons to both as far as I can see.&lt;/p&gt;
&lt;p&gt;I feel as though using statistical inference allows for more flexibility from a tuning perspective since you can tweak the thresholds, etc. However, at the same time that is a manual process that would need to be monitored/revisited.&lt;/p&gt;
&lt;p&gt;With KMeans, the opposite is somewhat true. There's less threshold tweaking involved, and more hyperparameter tuning to find which combinations optimally detect usage.&lt;/p&gt;
&lt;p&gt;If I had to choose which one I think did better in this exercise (w/o knowing the right answer) I would choose the &lt;strong&gt;KMeans&lt;/strong&gt; algorithm. While both methods captured the large spikes, there were some other medium-sized spikes that occurred in the data that seemd as though they could have been valid usage, and KMeans picked them up. These occurred at ~ 6am and ~12pm.&lt;/p&gt;
&lt;h3 id="problemsdifficultiesareasforimprovement"&gt;Problems/Difficulties/Areas for Improvement&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;More Data&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;I think this is a no-brainer, but is often times the case. The more data (particularly &amp;quot;normal/healthy&amp;quot; data) that can be collected the better. This would allow us to rely less on assumptions. In addition to just more records, including other variables I think could help. For example: ambient readings of the environment, location of the equipment, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Transforming for High Kurtosis&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;For me this is more of an &lt;em&gt;Area for Improvement&lt;/em&gt;. After differencing the data, I pointed out that the distribution was Leptokurtic. If there's a way to tranform that into more of a normal distriubution, I think it would have lead to slightly better detection. While I think both methods did good, they seemed a little tight/under-sensitive; i.e the start date is a little late, and the end date is a little early.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--kg-card-end: markdown--&gt;</content:encoded></item></channel></rss>