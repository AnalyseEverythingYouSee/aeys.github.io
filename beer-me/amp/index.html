
<head>
    <meta charset="utf-8">

    <title>Beer Me!</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon">
    <link rel="canonical" href="../index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Analyse Everything You See">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Beer Me!">
    <meta property="og:description" content="In this article, I take a data set of ~75,000 homemade beer recipes and see if i can predict the style of beer.">
    <meta property="og:url" content="http://localhost:2368/beer-me/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1504502350688-00f5d59bbdeb?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ">
    <meta property="article:published_time" content="2020-03-25T23:48:51.000Z">
    <meta property="article:modified_time" content="2020-03-26T18:02:34.000Z">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="R">
    
    <meta property="article:publisher" content="https://www.facebook.com/ghost">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Beer Me!">
    <meta name="twitter:description" content="In this article, I take a data set of ~75,000 homemade beer recipes and see if i can predict the style of beer.">
    <meta name="twitter:url" content="http://localhost:2368/beer-me/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1504502350688-00f5d59bbdeb?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Gary Clark Jr.">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Machine Learning, R">
    <meta name="twitter:site" content="@tryghost">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="1333">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Analyse Everything You See",
        "logo": "http://localhost:2368/content/images/2020/03/aeys.png"
    },
    "author": {
        "@type": "Person",
        "name": "Gary Clark Jr.",
        "image": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/content/images/2020/03/IMG_1116-3.jpg",
            "width": 505,
            "height": 608
        },
        "url": "http://localhost:2368/author/gary-clark-jr/",
        "sameAs": [
            "http://gclarkjr5.github.io/"
        ]
    },
    "headline": "Beer Me!",
    "url": "http://localhost:2368/beer-me/",
    "datePublished": "2020-03-25T23:48:51.000Z",
    "dateModified": "2020-03-26T18:02:34.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1504502350688-00f5d59bbdeb?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ",
        "width": 2000,
        "height": 1333
    },
    "keywords": "Machine Learning, R",
    "description": "In this article, I take a data set of ~75,000 homemade beer recipes and see if i can predict the style of beer.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.8">
    <link rel="alternate" type="application/rss+xml" title="Analyse Everything You See" href="../../rss/index.html">

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,600,400">
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../">Analyse Everything You See</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Beer Me!</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../author/gary-clark-jr/">Gary Clark Jr.</a></p>
                    <time class="post-date" datetime="2020-03-25">2020-03-25</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://images.unsplash.com/photo-1504502350688-00f5d59bbdeb?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <h3 id="context">Context</h3>
<p>This is a dataset of 75,000 homebrewed beers with over 176 different styles. Beer records are user-reported and are classified according to one of the 176 different styles. These recipes go into as much or as little detail as the user provided, but there's at least 5 useful columns where data was entered for each: Original Gravity, Final Gravity, ABV, IBU, and Color</p>
<h3 id="inspiration">Inspiration</h3>
<p>What goes into homemade beer?</p>
<p>It would be interesting to see if the data provided is enough to define each class or if there are undiscovered patterns. In the future it might be possible to go through and scrape more detailed information for each recipe, such as the yeast and specific hops used.</p>
<h2 id="problemstatement">Problem Statement</h2>
<p>Based on the above, let's attempt to predict beer style.</p>
<h2 id="intitialsetup">Intitial Setup</h2>
<h3 id="clearenvironment">Clear Environment</h3>
<pre><code class="language-R">rm(list = ls())

options(repr.plot.width=13, repr.plot.height=10)
</code></pre>
<h2 id="loadlibraries">Load Libraries</h2>
<pre><code class="language-R">library(tidyverse)
library(tidymodels)
library(skimr)
library(GGally)
</code></pre>
<h2 id="readindata">Read in Data</h2>
<pre><code class="language-R">beer_recipes = read_csv("recipeData.csv")
</code></pre>
<h1 id="intitialview">Intitial View</h1>
<p>Let's get a glimpse of what're working with here.</p>
<pre><code class="language-R">beer_recipes %&gt;% glimpse()
</code></pre>
<pre><code>Observations: 73,861
Variables: 23
$ BeerID        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …
$ Name          &lt;chr&gt; "Vanilla Cream Ale", "Southern Tier Pumking clone", "Zo…
$ URL           &lt;chr&gt; "/homebrew/recipe/view/1633/vanilla-cream-ale", "/homeb…
$ Style         &lt;chr&gt; "Cream Ale", "Holiday/Winter Special Spiced Beer", "Ame…
$ StyleID       &lt;dbl&gt; 45, 85, 7, 7, 20, 10, 86, 45, 129, 86, 7, 7, 7, 7, 7, 3…
$ `Size(L)`     &lt;dbl&gt; 21.77, 20.82, 18.93, 22.71, 50.00, 24.61, 22.71, 20.82,…
$ OG            &lt;dbl&gt; 1.055, 1.083, 1.063, 1.061, 1.060, 1.055, 1.072, 1.054,…
$ FG            &lt;dbl&gt; 1.013, 1.021, 1.018, 1.017, 1.010, 1.013, 1.018, 1.014,…
$ ABV           &lt;dbl&gt; 5.48, 8.16, 5.91, 5.80, 6.48, 5.58, 7.09, 5.36, 5.77, 8…
$ IBU           &lt;dbl&gt; 17.65, 60.65, 59.25, 54.48, 17.84, 40.12, 268.71, 19.97…
$ Color         &lt;dbl&gt; 4.83, 15.64, 8.98, 8.50, 4.57, 8.00, 6.33, 5.94, 34.76,…
$ BoilSize      &lt;dbl&gt; 28.39, 24.61, 22.71, 26.50, 60.00, 29.34, 30.28, 28.39,…
$ BoilTime      &lt;dbl&gt; 75, 60, 60, 60, 90, 70, 90, 75, 75, 60, 90, 90, 60, 60,…
$ BoilGravity   &lt;chr&gt; "1.038", "1.07", "N/A", "N/A", "1.05", "1.047", "N/A", …
$ Efficiency    &lt;dbl&gt; 70, 70, 70, 70, 72, 79, 75, 70, 73, 70, 74, 70, 70, 30,…
$ MashThickness &lt;chr&gt; "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "N/A", "1.4",…
$ SugarScale    &lt;chr&gt; "Specific Gravity", "Specific Gravity", "Specific Gravi…
$ BrewMethod    &lt;chr&gt; "All Grain", "All Grain", "extract", "All Grain", "All …
$ PitchRate     &lt;chr&gt; "N/A", "N/A", "N/A", "N/A", "N/A", "1", "N/A", "N/A", "…
$ PrimaryTemp   &lt;chr&gt; "17.78", "N/A", "N/A", "N/A", "19", "N/A", "N/A", "N/A"…
$ PrimingMethod &lt;chr&gt; "corn sugar", "N/A", "N/A", "N/A", "Sukkerlake", "N/A",…
$ PrimingAmount &lt;chr&gt; "4.5 oz", "N/A", "N/A", "N/A", "6-7 g sukker/l", "N/A",…
$ UserId        &lt;dbl&gt; 116, 955, NA, NA, 18325, 5889, 1051, 116, 116, NA, 1832…
</code></pre>
<p>Great! I see a few issues in the data that I already know I'm going to have to account for, but I'll go into more detail when I get there.</p>
<h1 id="datacleansing">Data Cleansing</h1>
<h2 id="usingarecipe">Using a Recipe</h2>
<p>In R, there is a concept called a <strong>recipe</strong> (nothing to do with the data I'm currently working with lol). It basically allows one to build incremental steps for cleaning/pre-processing the data to get it ready for analysis, modeling, etc. I personally like it because:</p>
<ol>
<li>
<p>It will list out the steps that I am performing on the data, and what variables are going to be affected by those steps.</p>
</li>
<li>
<p>It performs lazy evaluation, so it doesn't actually do anything to the data until you ask for it.</p>
</li>
</ol>
<p>So let's start off with initializing our recipe!</p>
<p>At this point I'm going to split our train and test set, our train set will guide us through all of the steps we need to perform in order to get our model ready, and our test set will act as new data that we've never seen before to help assess how well our model did on <strong>real data</strong>.</p>
<pre><code class="language-R">set.seed(0415)
split = initial_split(beer_recipes)

split

train = training(split)
test = testing(split)

beer_rec = train %&gt;% recipe()

beer_rec
</code></pre>
<pre><code>&lt;55396/18465/73861&gt;



Data Recipe

Inputs:

  23 variables (no declared roles)
</code></pre>
<p>So ~55,000 training records and ~18,000 testing records.</p>
<p>Some of the first things that stood out to me in the glimpse above are:</p>
<ol>
<li>
<p>There are values showing up as "N/A" and not being recognized as <strong>actual NA</strong> values in R.</p>
</li>
<li>
<p>BoilGravity and PrimaryTemp columns are character columns when they should be numeric. This is likely due to the "N/A" issue.</p>
</li>
</ol>
<p>Additionally, there are some standard things I'll take care of:</p>
<ol>
<li>
<p>Change all column names to lower case. This is mostly a matter of preference.</p>
</li>
<li>
<p>Change all character columns to factors (categorical). This is to help reduce memory.</p>
</li>
</ol>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_rename_at(everything(), fn = ~str_to_lower(.)) %&gt;% # change column names to lowercase
    step_rename(size_l = `size(l)`) %&gt;%
    step_mutate_at(all_nominal(), fn = ~na_if(., "N/A")) %&gt;% # convert "N/A" to NA
    step_mutate_at(boilgravity, primarytemp, fn = ~as.numeric(as.character(.)))

beer_rec
</code></pre>
<pre><code>Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
</code></pre>
<p>You can see above that the output of our recipe provides a nice simple way to understand what steps and in what order the recipe intends to perform on our data.</p>
<p>If we wanted to get an idea of what our data would look like as a result of our recipe <strong>so far</strong> we can do that. Just prepare the recipe and juice it!</p>
<pre><code class="language-R">beer_rec %&gt;% prep() %&gt;% juice() %&gt;% glimpse()
</code></pre>
<pre><code>Observations: 55,396
Variables: 23
$ beerid        &lt;dbl&gt; 1, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 16, 17, 20, 21, 22…
$ name          &lt;fct&gt; Vanilla Cream Ale, Zombie Dust Clone - EXTRACT, Zombie …
$ url           &lt;fct&gt; /homebrew/recipe/view/1633/vanilla-cream-ale, /homebrew…
$ style         &lt;fct&gt; Cream Ale, American IPA, American IPA, Belgian Blond Al…
$ styleid       &lt;dbl&gt; 45, 7, 7, 20, 10, 86, 45, 7, 7, 7, 7, 31, 134, 7, 65, 8…
$ size_l        &lt;dbl&gt; 21.77, 18.93, 22.71, 50.00, 24.61, 22.71, 20.82, 25.00,…
$ og            &lt;dbl&gt; 1.0550, 1.0630, 1.0610, 1.0600, 1.0550, 1.0720, 1.0540,…
$ fg            &lt;dbl&gt; 1.0130, 1.0180, 1.0170, 1.0100, 1.0130, 1.0180, 1.0140,…
$ abv           &lt;dbl&gt; 5.48, 5.91, 5.80, 6.48, 5.58, 7.09, 5.36, 6.63, 6.62, 7…
$ ibu           &lt;dbl&gt; 17.65, 59.25, 54.48, 17.84, 40.12, 268.71, 19.97, 64.26…
$ color         &lt;dbl&gt; 4.83, 8.98, 8.50, 4.57, 8.00, 6.33, 5.94, 7.78, 14.26, …
$ boilsize      &lt;dbl&gt; 28.39, 22.71, 26.50, 60.00, 29.34, 30.28, 28.39, 29.00,…
$ boiltime      &lt;dbl&gt; 75, 60, 60, 90, 70, 90, 75, 90, 90, 60, 60, 60, 60, 60,…
$ boilgravity   &lt;dbl&gt; 1.038, NA, NA, 1.050, 1.047, NA, 1.040, 1.055, NA, NA, …
$ efficiency    &lt;dbl&gt; 70, 70, 70, 72, 79, 75, 70, 74, 70, 70, 30, 75, 70, 70,…
$ mashthickness &lt;fct&gt; NA, NA, NA, NA, NA, NA, 1.4, NA, NA, NA, NA, NA, NA, 1.…
$ sugarscale    &lt;fct&gt; Specific Gravity, Specific Gravity, Specific Gravity, S…
$ brewmethod    &lt;fct&gt; All Grain, extract, All Grain, All Grain, All Grain, Al…
$ pitchrate     &lt;fct&gt; NA, NA, NA, NA, 1, NA, NA, 1, NA, NA, 0.35, NA, NA, NA,…
$ primarytemp   &lt;dbl&gt; 17.78, NA, NA, 19.00, NA, NA, NA, 18.00, NA, NA, 18.33,…
$ primingmethod &lt;fct&gt; corn sugar, NA, NA, Sukkerlake, NA, NA, corn sugar, Suk…
$ primingamount &lt;fct&gt; 4.5 oz, NA, NA, 6-7 g sukker/l, NA, NA, 4.2 oz, 6 5 g s…
$ userid        &lt;dbl&gt; 116, NA, NA, 18325, 5889, 1051, 116, 18325, NA, NA, 235…
</code></pre>
<p>There are some columns in here that don't provide any additional value to us since they are essentially id columns, or made redundant by other columns:</p>
<ol>
<li>
<p>Beerid is the id of every row</p>
</li>
<li>
<p>Name is going to be mostly unique to every row (this could prove useful to analyze, but for now I'll drop it)</p>
</li>
<li>
<p>URL is just the link to where the recipe lives</p>
</li>
<li>
<p>Styleid is already represented by Style</p>
</li>
<li>
<p>Userid is a variable that could have some value (particular users are more likely to input their favorite beers) but we will disregard it for now</p>
</li>
</ol>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_rm(
        beerid,
        name,
        url,
        styleid,
        userid
    ) # remove the variables mentioned above
</code></pre>
<pre><code class="language-R">beer_rec
beer_rec %&gt;% prep() %&gt;% juice() %&gt;% glimpse()
</code></pre>
<pre><code>Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid


Observations: 55,396
Variables: 18
$ style         &lt;fct&gt; Cream Ale, American IPA, American IPA, Belgian Blond Al…
$ size_l        &lt;dbl&gt; 21.77, 18.93, 22.71, 50.00, 24.61, 22.71, 20.82, 25.00,…
$ og            &lt;dbl&gt; 1.0550, 1.0630, 1.0610, 1.0600, 1.0550, 1.0720, 1.0540,…
$ fg            &lt;dbl&gt; 1.0130, 1.0180, 1.0170, 1.0100, 1.0130, 1.0180, 1.0140,…
$ abv           &lt;dbl&gt; 5.48, 5.91, 5.80, 6.48, 5.58, 7.09, 5.36, 6.63, 6.62, 7…
$ ibu           &lt;dbl&gt; 17.65, 59.25, 54.48, 17.84, 40.12, 268.71, 19.97, 64.26…
$ color         &lt;dbl&gt; 4.83, 8.98, 8.50, 4.57, 8.00, 6.33, 5.94, 7.78, 14.26, …
$ boilsize      &lt;dbl&gt; 28.39, 22.71, 26.50, 60.00, 29.34, 30.28, 28.39, 29.00,…
$ boiltime      &lt;dbl&gt; 75, 60, 60, 90, 70, 90, 75, 90, 90, 60, 60, 60, 60, 60,…
$ boilgravity   &lt;dbl&gt; 1.038, NA, NA, 1.050, 1.047, NA, 1.040, 1.055, NA, NA, …
$ efficiency    &lt;dbl&gt; 70, 70, 70, 72, 79, 75, 70, 74, 70, 70, 30, 75, 70, 70,…
$ mashthickness &lt;fct&gt; NA, NA, NA, NA, NA, NA, 1.4, NA, NA, NA, NA, NA, NA, 1.…
$ sugarscale    &lt;fct&gt; Specific Gravity, Specific Gravity, Specific Gravity, S…
$ brewmethod    &lt;fct&gt; All Grain, extract, All Grain, All Grain, All Grain, Al…
$ pitchrate     &lt;fct&gt; NA, NA, NA, NA, 1, NA, NA, 1, NA, NA, 0.35, NA, NA, NA,…
$ primarytemp   &lt;dbl&gt; 17.78, NA, NA, 19.00, NA, NA, NA, 18.00, NA, NA, 18.33,…
$ primingmethod &lt;fct&gt; corn sugar, NA, NA, Sukkerlake, NA, NA, corn sugar, Suk…
$ primingamount &lt;fct&gt; 4.5 oz, NA, NA, 6-7 g sukker/l, NA, NA, 4.2 oz, 6 5 g s…
</code></pre>
<h1 id="exploration">Exploration</h1>
<p>Now lets get a look at the quality of the data that remains. The skimr package has a function, skim, that provides a nice  30,000 ft view.</p>
<pre><code class="language-R">beer_skim = beer_rec %&gt;%
    prep() %&gt;%
    juice() %&gt;%
    skim()
</code></pre>
<pre><code class="language-R">beer_skim
</code></pre>
<pre><code>── Data Summary ────────────────────────
                           Values    
Name                       Piped data
Number of rows             55396     
Number of columns          18        
_______________________              
Column type frequency:               
  factor                   7         
  numeric                  11        
________________________             
Group variables            None      

── Variable type: factor ───────────────────────────────────────────────────────
  skim_variable n_missing complete_rate ordered n_unique
1 style               465        0.992  FALSE        175
2 mashthickness     22380        0.596  FALSE        496
3 sugarscale            0        1      FALSE          2
4 brewmethod            0        1      FALSE          4
5 pitchrate         29239        0.472  FALSE          9
6 primingmethod     50274        0.0925 FALSE        722
7 primingamount     51802        0.0649 FALSE       1559
  top_counts                                 
1 Ame: 8968, Ame: 5677, Sai: 1953, Ame: 1678 
2 1.5: 11560, 3: 6296, 1.2: 3679, 2.5: 1421  
3 Spe: 53976, Pla: 1420                      
4 All: 37264, BIA: 8908, ext: 6522, Par: 2702
5 0.3: 7139, 0.7: 6840, 0.5: 4119, 1: 3992   
6 Cor: 546, Dex: 389, cor: 267, Keg: 247     
7 5 o: 153, 3/4: 79, 4 o: 78, 1 c: 74        

── Variable type: numeric ──────────────────────────────────────────────────────
   skim_variable n_missing complete_rate  mean      sd      p0   p25   p50   p75
 1 size_l                0         1     43.4  174.      1     18.9  20.8  23.7 
 2 og                    0         1      1.40   2.18    1      1.05  1.06  1.07
 3 fg                    0         1      1.08   0.432  -0.003  1.01  1.01  1.02
 4 abv                   0         1      6.14   1.90    0      5.08  5.79  6.83
 5 ibu                   0         1     44.3   43.3     0     23.4  35.8  56.4 
 6 color                 0         1     13.4   11.9     0      5.17  8.44 16.8 
 7 boilsize              0         1     49.2  186.      1     20.8  27.4  30   
 8 boiltime              0         1     65.1   15.1     0     60    60    60   
 9 boilgravity        2246         0.959  1.35   1.92    0      1.04  1.05  1.06
10 efficiency            0         1     66.3   14.1     0     65    70    75   
11 primarytemp       16963         0.694 19.2    4.21  -17.8   18    20    20   
     p100 hist 
 1 7800   ▇▁▁▁▁
 2   32.5 ▇▁▁▁▁
 3   23.4 ▇▁▁▁▁
 4   54.7 ▇▁▁▁▁
 5 3409.  ▇▁▁▁▁
 6  186   ▇▁▁▁▁
 7 7800   ▇▁▁▁▁
 8  240   ▁▇▁▁▁
 9   52.6 ▇▁▁▁▁
10  100   ▁▂▁▇▁
11  114   ▁▇▁▁▁
</code></pre>
<p>One last cleanup step we can do is removing variables with a lot of missing data. Several variables from above have low completion rates. Lets set a threshold of 60%, meaning we will remove variables with less than 60% completion rates. This is a personal threshold that I initially choose to set. This threshold could very well be adjusted.</p>
<pre><code class="language-R">vars_remove = beer_skim %&gt;%
    filter(complete_rate &lt; 0.6) %&gt;% # collect variables with less than 60% completion rate
    select(skim_variable) %&gt;%
    .[[1]]

beer_rec = beer_rec %&gt;%
    step_rm(one_of(vars_remove))
</code></pre>
<pre><code class="language-R">beer_rec
beer_rec %&gt;% prep() %&gt;% juice() %&gt;% skim()
</code></pre>
<pre><code>Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid
Delete terms one_of, vars_remove


── Data Summary ────────────────────────
                           Values    
Name                       Piped data
Number of rows             55396     
Number of columns          14        
_______________________              
Column type frequency:               
  factor                   3         
  numeric                  11        
________________________             
Group variables            None      

── Variable type: factor ───────────────────────────────────────────────────────
  skim_variable n_missing complete_rate ordered n_unique
1 style               465         0.992 FALSE        175
2 sugarscale            0         1     FALSE          2
3 brewmethod            0         1     FALSE          4
  top_counts                                 
1 Ame: 8968, Ame: 5677, Sai: 1953, Ame: 1678 
2 Spe: 53976, Pla: 1420                      
3 All: 37264, BIA: 8908, ext: 6522, Par: 2702

── Variable type: numeric ──────────────────────────────────────────────────────
   skim_variable n_missing complete_rate  mean      sd      p0   p25   p50   p75
 1 size_l                0         1     43.4  174.      1     18.9  20.8  23.7 
 2 og                    0         1      1.40   2.18    1      1.05  1.06  1.07
 3 fg                    0         1      1.08   0.432  -0.003  1.01  1.01  1.02
 4 abv                   0         1      6.14   1.90    0      5.08  5.79  6.83
 5 ibu                   0         1     44.3   43.3     0     23.4  35.8  56.4 
 6 color                 0         1     13.4   11.9     0      5.17  8.44 16.8 
 7 boilsize              0         1     49.2  186.      1     20.8  27.4  30   
 8 boiltime              0         1     65.1   15.1     0     60    60    60   
 9 boilgravity        2246         0.959  1.35   1.92    0      1.04  1.05  1.06
10 efficiency            0         1     66.3   14.1     0     65    70    75   
11 primarytemp       16963         0.694 19.2    4.21  -17.8   18    20    20   
     p100 hist 
 1 7800   ▇▁▁▁▁
 2   32.5 ▇▁▁▁▁
 3   23.4 ▇▁▁▁▁
 4   54.7 ▇▁▁▁▁
 5 3409.  ▇▁▁▁▁
 6  186   ▇▁▁▁▁
 7 7800   ▇▁▁▁▁
 8  240   ▁▇▁▁▁
 9   52.6 ▇▁▁▁▁
10  100   ▁▂▁▇▁
11  114   ▁▇▁▁▁
</code></pre>
<p>Cool, now let's start diving a little deeper into our remaining variables.</p>
<h2 id="dependentvariables">Dependent Variables</h2>
<p>Let's take a look at our dependent variable, style, that we intend on predicting.</p>
<ol>
<li>
<p>Missing data: while its possible that the values could be missing at random (MAR), maybe its possible that when the recipe was input by the user, the style classification that they intended on using was not available so they left it blank, which is intentional and is the assumption I'm going to use meaning that missing values should be their own classification</p>
</li>
<li>
<p>In the profile above we can see that there are 100+ styles available for us to classify, and the frequency of the top 4 starts off at ~9,000 and quickly drops to the next frequent of 5,600 then to the 2,000s and so on. That's almost 1/3 of the data grouped in the top 4 styles. We could group everything else into an "other" category, so let's choose a threshold that if a style occurs less than 1500 times in our data, that it get's classified as such.</p>
</li>
</ol>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_mutate(style = fct_explicit_na(style)) %&gt;% # turn NA values into their own classification
    step_other(style, threshold = 1500) # group classes appearing less than 2000 times into an "other" group
</code></pre>
<pre><code class="language-R">beer_rec
beer_rec %&gt;% prep() %&gt;% juice() %&gt;% count(style) # count the frequency of remaining styles
</code></pre>
<pre><code>Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid
Delete terms one_of, vars_remove
Variable mutation for  style
Collapsing factor levels for style
</code></pre>
<table>
<caption>A tibble: 6 × 2</caption>
<thead>
	<tr><th scope="col">style</th><th scope="col">n</th></tr>
	<tr><th scope="col">&lt;fct&gt;</th><th scope="col">&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><td>American Amber Ale  </td><td> 1532</td></tr>
	<tr><td>American IPA        </td><td> 8968</td></tr>
	<tr><td>American Light Lager</td><td> 1678</td></tr>
	<tr><td>American Pale Ale   </td><td> 5677</td></tr>
	<tr><td>Saison              </td><td> 1953</td></tr>
	<tr><td>other               </td><td>35588</td></tr>
</tbody>
</table>
<p>Nice, now the problem is that because we have so many styles with less than 1500 entries, all of them pooled together is still an overwhelimg amount. This causes a class imbalance, so essentially our model (depending on the one used) could become bias to choosing <strong>other</strong> since it's learned on more data that is classified as <strong>other</strong> compared to the alternate available labels. One trick to accomodate for this is to downsample our data so that we end up with relatively similar amounts of rows for each style, i.e giving all classes an equal opportunity to be learned by the model.</p>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_downsample(style)
</code></pre>
<pre><code class="language-R">beer_rec
beer_rec  %&gt;% prep() %&gt;% juice() %&gt;% count(style)
</code></pre>
<pre><code>Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid
Delete terms one_of, vars_remove
Variable mutation for  style
Collapsing factor levels for style
Down-sampling based on style
</code></pre>
<table>
<caption>A tibble: 6 × 2</caption>
<thead>
	<tr><th scope="col">style</th><th scope="col">n</th></tr>
	<tr><th scope="col">&lt;fct&gt;</th><th scope="col">&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><td>American Amber Ale  </td><td>1532</td></tr>
	<tr><td>American IPA        </td><td>1532</td></tr>
	<tr><td>American Light Lager</td><td>1532</td></tr>
	<tr><td>American Pale Ale   </td><td>1532</td></tr>
	<tr><td>Saison              </td><td>1532</td></tr>
	<tr><td>other               </td><td>1532</td></tr>
</tbody>
</table>
<p>Even Steven!</p>
<h2 id="independentvariables">Independent Variables</h2>
<p>Looking at our most recent profiling (skim) of the data above, we still have some missing data for both boilgravity and primarytemp. These measurements seem to actually be missing at random; both appear to be measurements that are taken at particular times in the brewing process and could have easily been missed. With this info, we know we will have to impute these variables</p>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_bagimpute(boilgravity, primarytemp)
</code></pre>
<p>Now let's take a look at the distributions of our variables, as well as what they look like against each other.</p>
<pre><code class="language-R">beer_rec %&gt;%
    prep() %&gt;%
    juice() %&gt;%
    select(-style) %&gt;%
    ggpairs()
</code></pre>
<p></p>
<p>I know this plot is a little bit messy, but just to point out a few things I notice... Looking at the variables individually, there appears to be some skew amongst all of them, so we can adjust this with a transformation like BoxCox or YeoJohnson.</p>
<p>Regarding the categorical vars, sugarscale is heavily imbalanced.</p>
<ul>
<li>I learned that brewers can define the specific gravity of the beer, but some like to use what is known as the "Plato" scale. Since we are here, lets take this opportunity to convert gravity measurements with Plato values to Specifc Gravity. SG = (4P/1000) + 1</li>
</ul>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_YeoJohnson(all_numeric()) %&gt;% # remove some skew from skewed vars
    step_mutate_at(fg, og, boilgravity, fn = ~((4 * .)/1000) + 1) %&gt;% # convert final gravity using plato values to specific gravity
    step_rm(sugarscale) # remove sugarscale column because now specific gravity is the only value    
</code></pre>
<pre><code class="language-R">beer_rec
beer_rec %&gt;% prep() %&gt;% juice() %&gt;% glimpse()
</code></pre>
<pre><code>Data Recipe

Inputs:

  23 variables (no declared roles)

Operations:

Variable renaming for everything
Variable renaming for  size_l
Variable mutation for all_nominal
Variable mutation for boilgravity, primarytemp
Delete terms beerid, name, url, styleid, userid
Delete terms one_of, vars_remove
Variable mutation for  style
Collapsing factor levels for style
Down-sampling based on style
Bagged tree imputation for boilgravity, primarytemp
Yeo-Johnson transformation on all_numeric
Variable mutation for fg, og, boilgravity
Delete terms sugarscale


Observations: 9,192
Variables: 13
$ style       &lt;fct&gt; American Amber Ale, American Amber Ale, American Amber Al…
$ size_l      &lt;dbl&gt; 1.775784, 1.803888, 1.803888, 1.886012, 1.855547, 1.82880…
$ og          &lt;dbl&gt; 1.004300, 1.004240, 1.004240, 1.004220, 1.004260, 1.00426…
$ fg          &lt;dbl&gt; 1.004036, 1.004064, 1.004068, 1.004048, 1.004060, 1.00406…
$ abv         &lt;dbl&gt; 2.850152, 2.453214, 2.433714, 2.479746, 2.661498, 2.66591…
$ ibu         &lt;dbl&gt; 0.000000, 5.781126, 7.438330, 9.561051, 11.584073, 10.097…
$ color       &lt;dbl&gt; 1.669649, 2.202502, 1.691027, 2.022844, 1.962021, 1.98490…
$ boilsize    &lt;dbl&gt; 1.898764, 1.955817, 1.661273, 1.961717, 1.971877, 2.00135…
$ boiltime    &lt;dbl&gt; 72.25711, 49.32056, 41.53138, 25.63932, 72.25711, 49.3205…
$ boilgravity &lt;dbl&gt; 1.004210, 1.004180, 1.004210, 1.004220, 1.004224, 1.00418…
$ efficiency  &lt;dbl&gt; 178237.01, 220129.90, 13632.68, 220129.90, 211268.70, 194…
$ brewmethod  &lt;fct&gt; All Grain, All Grain, extract, BIAB, All Grain, All Grain…
$ primarytemp &lt;dbl&gt; 18.67447, 19.22292, 18.68587, 17.32834, 17.32834, 18.6628…
</code></pre>
<p>We now have one categorical independent variable left, brewmethod. Depending on the type of modelling technique we use, this variable would have to be dummy/one-hot encoded. A model like KNN would require this, but a random forest would not. It shouldn't hurt though, and since I intend on trying KNN, lets one-hot encode it.</p>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_dummy(all_nominal(), -style, one_hot = TRUE)
</code></pre>
<pre><code class="language-R">beer_rec %&gt;% prep() %&gt;% juice() %&gt;% glimpse()
</code></pre>
<pre><code>Observations: 9,192
Variables: 16
$ style                   &lt;fct&gt; American Amber Ale, American Amber Ale, Ameri…
$ size_l                  &lt;dbl&gt; 1.775784, 1.803888, 1.803888, 1.886012, 1.855…
$ og                      &lt;dbl&gt; 1.004300, 1.004240, 1.004240, 1.004220, 1.004…
$ fg                      &lt;dbl&gt; 1.004036, 1.004064, 1.004068, 1.004048, 1.004…
$ abv                     &lt;dbl&gt; 2.850152, 2.453214, 2.433714, 2.479746, 2.661…
$ ibu                     &lt;dbl&gt; 0.000000, 5.781126, 7.438330, 9.561051, 11.58…
$ color                   &lt;dbl&gt; 1.669649, 2.202502, 1.691027, 2.022844, 1.962…
$ boilsize                &lt;dbl&gt; 1.898764, 1.955817, 1.661273, 1.961717, 1.971…
$ boiltime                &lt;dbl&gt; 72.25711, 49.32056, 41.53138, 25.63932, 72.25…
$ boilgravity             &lt;dbl&gt; 1.004210, 1.004180, 1.004210, 1.004220, 1.004…
$ efficiency              &lt;dbl&gt; 178237.01, 220129.90, 13632.68, 220129.90, 21…
$ primarytemp             &lt;dbl&gt; 18.67447, 19.22292, 18.68587, 17.32834, 17.32…
$ brewmethod_All.Grain    &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, …
$ brewmethod_BIAB         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, …
$ brewmethod_extract      &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
$ brewmethod_Partial.Mash &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
</code></pre>
<p>Coming into some of the final pre-processing steps we can utilize, lets see if any of our remaining independent variables have any <strong>Near-Zero Variance</strong>. These sparse &amp; highly imbalanced IVs have low predictive power due to their lack of variation/occurrence relative to the different styles that occur, so let's filter them out.</p>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_nzv(all_numeric())
</code></pre>
<pre><code class="language-R">beer_rec %&gt;% prep() %&gt;% juice() %&gt;% glimpse()
</code></pre>
<pre><code>Observations: 9,192
Variables: 15
$ style                &lt;fct&gt; American Amber Ale, American Amber Ale, American…
$ size_l               &lt;dbl&gt; 1.775784, 1.803888, 1.803888, 1.886012, 1.855547…
$ og                   &lt;dbl&gt; 1.004300, 1.004240, 1.004240, 1.004220, 1.004260…
$ fg                   &lt;dbl&gt; 1.004036, 1.004064, 1.004068, 1.004048, 1.004060…
$ abv                  &lt;dbl&gt; 2.850152, 2.453214, 2.433714, 2.479746, 2.661498…
$ ibu                  &lt;dbl&gt; 0.000000, 5.781126, 7.438330, 9.561051, 11.58407…
$ color                &lt;dbl&gt; 1.669649, 2.202502, 1.691027, 2.022844, 1.962021…
$ boilsize             &lt;dbl&gt; 1.898764, 1.955817, 1.661273, 1.961717, 1.971877…
$ boiltime             &lt;dbl&gt; 72.25711, 49.32056, 41.53138, 25.63932, 72.25711…
$ boilgravity          &lt;dbl&gt; 1.004210, 1.004180, 1.004210, 1.004220, 1.004224…
$ efficiency           &lt;dbl&gt; 178237.01, 220129.90, 13632.68, 220129.90, 21126…
$ primarytemp          &lt;dbl&gt; 18.67447, 19.22292, 18.68587, 17.32834, 17.32834…
$ brewmethod_All.Grain &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, …
$ brewmethod_BIAB      &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, …
$ brewmethod_extract   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
</code></pre>
<p>Next lets see if any issues of high correlation are present.</p>
<pre><code class="language-R">beer_rec %&gt;%
    prep() %&gt;%
    juice() %&gt;%
    select_if(is.numeric) %&gt;%
    select_at(vars(-starts_with("brewmethod"))) %&gt;%
    ggcorr(nbreaks = 5, label = TRUE) # look to see potential multicollinearity
</code></pre>
<p></p>
<p>Clearly we have some high correlations amongst a few variables:</p>
<ul>
<li>boilgravity vs fg</li>
<li>boilgravity vs og</li>
<li>boilsize vs size_l</li>
<li>fg vs og</li>
</ul>
<p>Now we can implement a filter to remove highly correlated variables, which would imply that the variables may be redundant of each other. Additionally, after removing highly correlated variables, let's also remove any variables that may be linear combinations of each other, implying a potential issue with multicollinearity.</p>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_corr(all_numeric(), threshold = 0.8) %&gt;%
    step_lincomb(all_numeric())
</code></pre>
<p>Lastly, because several of our continuous numerical variables live within different scales of each other, let's normalize them all to bring them within the same space.</p>
<pre><code class="language-R">beer_rec = beer_rec %&gt;%
    step_normalize(all_numeric(), -starts_with("brewmethod"))
</code></pre>
<pre><code class="language-R">beer_rec %&gt;% prep() %&gt;% juice() %&gt;% glimpse()
</code></pre>
<pre><code>Observations: 9,192
Variables: 12
$ style                &lt;fct&gt; American Amber Ale, American Amber Ale, American…
$ size_l               &lt;dbl&gt; -0.20380362, -0.05002251, -0.05002251, 0.3993566…
$ fg                   &lt;dbl&gt; -0.1531746, -0.1348155, -0.1321928, -0.1453064, …
$ abv                  &lt;dbl&gt; 1.02733267, -0.06087288, -0.11433241, 0.01186519…
$ ibu                  &lt;dbl&gt; -2.27735546, -0.73114821, -0.28791631, 0.2798221…
$ color                &lt;dbl&gt; 0.105442916, 1.797271835, 0.173318912, 1.2268501…
$ boiltime             &lt;dbl&gt; 2.000994, -0.296342, -1.076510, -2.668266, 2.000…
$ efficiency           &lt;dbl&gt; 0.1164106, 0.6481222, -1.9727751, 0.6481222, 0.5…
$ primarytemp          &lt;dbl&gt; -0.12042289, 0.05232143, -0.11683074, -0.5444035…
$ brewmethod_All.Grain &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, …
$ brewmethod_BIAB      &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, …
$ brewmethod_extract   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
</code></pre>
<p>Let's get one last look at our final recipe.</p>
<pre><code class="language-R">beer_rec %&gt;% prep()
</code></pre>
<pre><code>Data Recipe

Inputs:

  23 variables (no declared roles)

Training data contained 55396 data points and 37817 incomplete rows. 

Operations:

Variable renaming for BeerID, Name, URL, Style, StyleID, ... [trained]
Variable renaming for  size_l [trained]
Variable mutation for name, url, style, ... [trained]
Variable mutation for boilgravity, primarytemp [trained]
Variables removed beerid, name, url, styleid, userid [trained]
Variables removed mashthickness, pitchrate, primingmethod, primingamount [trained]
Variable mutation for  style [trained]
Collapsing factor levels for style [trained]
Down-sampling based on style [trained]
Bagged tree imputation for boilgravity, primarytemp [trained]
Yeo-Johnson transformation on size_l, abv, ibu, color, ... [trained]
Variable mutation for fg, og, boilgravity [trained]
Variables removed sugarscale [trained]
Dummy variables from brewmethod [trained]
Sparse, unbalanced variable filter removed brewmethod_Partial.Mash [trained]
Correlation filter removed boilsize, boilgravity, og [trained]
Linear combination filter removed no terms [trained]
Centering and scaling for size_l, fg, abv, ibu, color, ... [trained]
</code></pre>
<h1 id="modeling">Modeling</h1>
<p>Let's bake our recipe into our test data. Remember, our test data hasn't seen any of these steps since we are treating it as brand new data.</p>
<pre><code class="language-R">test_data = beer_rec %&gt;%
    prep() %&gt;%
    bake(test)
</code></pre>
<p>Let's try out several different models:</p>
<ul>
<li>K Nearest Neighbours (KNN)</li>
<li>Decision Tree</li>
<li>Random Forest</li>
<li>XGBoost</li>
</ul>
<pre><code class="language-R">train_data = beer_rec %&gt;% prep() %&gt;% juice()

set.seed(0415)

knn = nearest_neighbor() %&gt;%
    set_engine("kknn") %&gt;%
    set_mode("classification")

knn_fit = knn %&gt;%
    fit(style ~ ., train_data)

set.seed(0415)

dectree = decision_tree() %&gt;%
    set_engine("rpart") %&gt;%
    set_mode("classification")

dectree_fit = dectree %&gt;%
    fit(style ~ ., train_data)

# Ensembles
set.seed(0415)

rf = rand_forest() %&gt;%
    set_engine("randomForest") %&gt;%
    set_mode("classification")

rf_fit = rf %&gt;%
    fit(style ~ ., train_data)

set.seed(0415)

xgb = boost_tree() %&gt;%
    set_engine("xgboost") %&gt;%
    set_mode("classification")

xgb_fit = xgb %&gt;%
    fit(style ~ ., train_data)
</code></pre>
<pre><code class="language-R">knn_fit
dectree_fit
rf_fit
xgb_fit
</code></pre>
<pre><code>parsnip model object

Fit time:  3.3s 

Call:
kknn::train.kknn(formula = formula, data = data, ks = 5)

Type of response variable: nominal
Minimal misclassification: 0.5039164
Best kernel: optimal
Best k: 5



parsnip model object

Fit time:  533ms 
n= 9192 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 9192 7660 American Amber Ale (0.17 0.17 0.17 0.17 0.17 0.17)  
   2) primarytemp&lt; 0.7528845 8089 6593 American IPA (0.18 0.18 0.18 0.18 0.083 0.18)  
     4) color&gt;=0.4493611 2859 1625 American Amber Ale (0.43 0.084 0.14 0.053 0.021 0.27)  
       8) color&lt; 1.398371 2125  961 American Amber Ale (0.55 0.095 0.096 0.066 0.021 0.17) *
       9) color&gt;=1.398371 734  327 other (0.095 0.054 0.26 0.014 0.022 0.55) *
     5) color&lt; 0.4493611 5230 3891 American Pale Ale (0.048 0.24 0.21 0.26 0.12 0.13)  
      10) ibu&gt;=0.5376232 1459  569 American IPA (0.027 0.61 0.12 0.15 0.019 0.071) *
      11) ibu&lt; 0.5376232 3771 2658 American Pale Ale (0.056 0.097 0.24 0.3 0.15 0.16)  
        22) ibu&lt; -0.612908 932  426 American Light Lager (0.033 0.05 0.54 0.064 0.068 0.24) *
        23) ibu&gt;=-0.612908 2839 1786 American Pale Ale (0.064 0.11 0.14 0.37 0.18 0.13)  
          46) primarytemp&lt; 0.3408113 2305 1367 American Pale Ale (0.067 0.12 0.16 0.41 0.1 0.15) *
          47) primarytemp&gt;=0.3408113 534  247 Saison (0.051 0.082 0.051 0.22 0.54 0.064) *
   3) primarytemp&gt;=0.7528845 1103  244 Saison (0.042 0.033 0.056 0.038 0.78 0.053) *



parsnip model object

Fit time:  17.2s 

Call:
 randomForest(x = as.data.frame(x), y = y) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 3

        OOB estimate of  error rate: 30.36%
Confusion matrix:
                     American Amber Ale American IPA American Light Lager
American Amber Ale                 1151           83                   23
American IPA                         97         1073                   29
American Light Lager                 64           91                 1040
American Pale Ale                   138          242                   49
Saison                               48           55                   25
other                               166           94                  123
                     American Pale Ale Saison other class.error
American Amber Ale                 123     48   104   0.2486945
American IPA                       213     56    64   0.2996084
American Light Lager                96     90   151   0.3211488
American Pale Ale                  935    107    61   0.3896867
Saison                              87   1254    63   0.1814621
other                              107     94   948   0.3812010



parsnip model object

Fit time:  1.6s 
##### xgb.Booster
raw: 295.8 Kb 
call:
  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, 
    colsample_bytree = 1, min_child_weight = 1, subsample = 1), 
    data = x, nrounds = 15, verbose = 0, objective = "multi:softprob", 
    num_class = 6L, nthread = 1)
params (as set within xgb.train):
  eta = "0.3", max_depth = "6", gamma = "0", colsample_bytree = "1", min_child_weight = "1", subsample = "1", objective = "multi:softprob", num_class = "6", nthread = "1", silent = "1"
xgb.attributes:
  niter
# of features: 11 
niter: 15
nfeatures : 11 
</code></pre>
<h1 id="evaluatingmodel">Evaluating model</h1>
<p>So we've trained our models. However, we can potentially improve their predictabilities using <strong>Cross-Fold Validation</strong>. The idea here is to give the models different looks of the data by creating multiple data sets that are essentially different subsets of the original training data. We then train the data on all of these new sets, evaluate the performance and select the model based on a particular criteria.</p>
<pre><code class="language-R">folds = vfold_cv(train_data, strata = style)
glimpse(folds)
</code></pre>
<pre><code>Observations: 10
Variables: 2
$ splits &lt;named list&gt; [&lt;rsplit[8268 x 924 x 9192 x 12]&gt;, &lt;rsplit[8268 x 924 x…
$ id     &lt;chr&gt; "Fold01", "Fold02", "Fold03", "Fold04", "Fold05", "Fold06", "F…
</code></pre>
<pre><code class="language-R">knn_res = fit_resamples(
    style ~ .,
    knn,
    folds,
    control = control_resamples(save_pred = TRUE)
)

dectree_res = fit_resamples(
    style ~ .,
    dectree,
    folds,
    control = control_resamples(save_pred = TRUE)
)

rf_res = fit_resamples(
    style ~ .,
    rf,
    folds,
    control = control_resamples(save_pred = TRUE)
)

xgb_res = fit_resamples(
    style ~ .,
    xgb,
    folds,
    control = control_resamples(save_pred = TRUE)
)
</code></pre>
<pre><code class="language-R">knn_res %&gt;%
    collect_metrics() %&gt;%
    mutate(model = "knn") %&gt;%
    bind_rows(
        dectree_res %&gt;%
            collect_metrics() %&gt;%
            mutate(model = "dectree")
    ) %&gt;%
    bind_rows(
        rf_res %&gt;%
            collect_metrics() %&gt;%
            mutate(model = "rf")
    ) %&gt;%
    bind_rows(
        xgb_res %&gt;%
            collect_metrics() %&gt;%
            mutate(model = "xgb")
    ) %&gt;%
    arrange(.metric, desc(mean))
</code></pre>
<table>
<caption>A tibble: 8 × 6</caption>
<thead>
	<tr><th scope="col">.metric</th><th scope="col">.estimator</th><th scope="col">mean</th><th scope="col">n</th><th scope="col">std_err</th><th scope="col">model</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;int&gt;</th><th scope="col">&lt;dbl&gt;</th><th scope="col">&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>accuracy</td><td>multiclass</td><td>0.7089834</td><td>10</td><td>0.005538325</td><td>xgb    </td></tr>
	<tr><td>accuracy</td><td>multiclass</td><td>0.6969004</td><td>10</td><td>0.005536357</td><td>rf     </td></tr>
	<tr><td>accuracy</td><td>multiclass</td><td>0.5440306</td><td>10</td><td>0.006265448</td><td>dectree</td></tr>
	<tr><td>accuracy</td><td>multiclass</td><td>0.4919383</td><td>10</td><td>0.003866652</td><td>knn    </td></tr>
	<tr><td>roc_auc </td><td>hand_till </td><td>0.9262713</td><td>10</td><td>0.002184205</td><td>xgb    </td></tr>
	<tr><td>roc_auc </td><td>hand_till </td><td>0.9140980</td><td>10</td><td>0.002991242</td><td>rf     </td></tr>
	<tr><td>roc_auc </td><td>hand_till </td><td>0.7923928</td><td>10</td><td>0.003745520</td><td>knn    </td></tr>
	<tr><td>roc_auc </td><td>hand_till </td><td>0.7904161</td><td>10</td><td>0.004375007</td><td>dectree</td></tr>
</tbody>
</table>
<p>We see that XGBoost outperformed the others, barely beating out random forest. Let's visualize the ROC curve of all.</p>
<p>Also, let's see the confusion matrix for our winner.</p>
<pre><code class="language-R">knn_res %&gt;%
    unnest(.predictions) %&gt;%
    mutate(model = "knn") %&gt;%
    bind_rows(
        dectree_res %&gt;%
            unnest(.predictions) %&gt;%
            mutate(model = "dectree")
    ) %&gt;%
    bind_rows(
        rf_res %&gt;%
            unnest(.predictions) %&gt;%
            mutate(model = "rf")
    ) %&gt;%
    bind_rows(
        xgb_res %&gt;%
            unnest(.predictions) %&gt;%
            mutate(model = "xgb")
    ) %&gt;%
    group_by(model) %&gt;%
    roc_curve(starts_with(".pred_"), -.pred_class, truth = style) %&gt;%
    autoplot()

# Confusion Matrix for XGBoost
xgb_res %&gt;%
    unnest(.predictions) %&gt;%
    conf_mat(style, .pred_class) %&gt;%
    autoplot(type = "heatmap")
</code></pre>
<p></p>
<p></p>
<p>Now, we will test XGBoost against our <strong>test</strong> data to see how well it performs on unseen data!</p>
<pre><code class="language-R">class_metrics = metric_set(accuracy)

xgb_fit %&gt;%
    predict(new_data = test_data) %&gt;%
    class_metrics(truth = test_data$style, estimate = .pred_class)
</code></pre>
<table>
<caption>A tibble: 1 × 3</caption>
<thead>
	<tr><th scope="col">.metric</th><th scope="col">.estimator</th><th scope="col">.estimate</th></tr>
	<tr><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;chr&gt;</th><th scope="col">&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>accuracy</td><td>multiclass</td><td>0.6631465</td></tr>
</tbody>
</table>
<p>66% of the new styles were correctly classified. That's ok. Not as good as we performed on the training data, but not too far away either.</p>
<p>Some possible areas of improvement:</p>
<ol>
<li>Investigating if there is any value that could be pulled out of the name of the beer.</li>
<li>Adjusting the <strong>missing data threshold</strong> for removing variables.</li>
<li>Analyzing the imputation methods deeper.</li>
<li>Adjusting the style groupings to include more/less classes</li>
<li>Hyperparameter Tuning?</li>
<li>Trying out other models.</li>
</ol>
<p>There are many other things that could potentially be improved, but these are just to note a few. Maybe I have some new blog posts now :).</p>
<p>Thanks.</p>


            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../">Analyse Everything You See</a> © 2020</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
